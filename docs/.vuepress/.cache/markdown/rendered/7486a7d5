{"content":"<h2 id=\"视觉模型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#视觉模型\"><span>视觉模型</span></a></h2>\n<h3 id=\"分类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#分类\"><span>分类</span></a></h3>\n<h4 id=\"任务\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#任务\"><span>任务</span></a></h4>\n<ul>\n<li>\n<p>图像分类：识别出图中出现的物体类别是什么，其功能主要是用于判断是什么？</p>\n<ul>\n<li>VGG</li>\n<li>GoogleNet</li>\n<li>ResNet</li>\n</ul>\n</li>\n<li>\n<p>图像定位：不仅仅需要识别出是什么物体（即分类）同时需要预测物体的位置信息，也就是单个目标在哪里？是什么？</p>\n<ul>\n<li>RCNN</li>\n<li>Fast RCNN</li>\n<li>Faster RCNN</li>\n</ul>\n</li>\n<li>\n<p>目标检测：多目标的定位，即在一个图片中定位多个目标物体，包括分类和定位，也就是多个目标分别在哪里？分别属于那个类别？</p>\n<ul>\n<li>RCNN</li>\n<li>Fast RNN</li>\n<li>Faster RCNN</li>\n<li>SSD</li>\n<li>YOLO</li>\n</ul>\n</li>\n</ul>\n<img src=\"@source/blog/深度学习模型.assets/image-20250405172047846.png\" alt=\"image-20250405172047846\" style=\"zoom:80%;\" />\n<h4 id=\"模型架构\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型架构\"><span>模型架构</span></a></h4>\n<ul>\n<li>二阶段：R-CNN、Fast R-CNN、Faster-R-CNN、SPP-Net、R-FCN</li>\n<li>一阶段：YOLO、SSD、FPN</li>\n</ul>\n<p>图像分割与目标检测：Cascade R-CNN</p>\n<h3 id=\"参数介绍\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#参数介绍\"><span>参数介绍</span></a></h3>\n<h4 id=\"iou\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#iou\"><span>IOU</span></a></h4>\n<p><code v-pre>两个边界框(bounding box)的重叠度</code></p>\n<img src=\"@source/blog/深度学习模型.assets/image-20250405175924671.png\" alt=\"image-20250405175924671\" style=\"zoom:50%;\" />\n$$\nIOU=\\frac{A\\cap B}{A\\cup B}=\\frac{S_{_{A,B}}}{S_{_A}+S_{_B}-S_{_{A,B}}}\n$$\n<h4 id=\"map\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#map\"><span>MAP</span></a></h4>\n<p><strong>精度和召回率</strong></p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>正例（实际）</th>\n<th>负例（实际）</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>正例（预测）</strong></td>\n<td>TP</td>\n<td>FP</td>\n</tr>\n<tr>\n<td><strong>负例（预测）</strong></td>\n<td>FN</td>\n<td>TN</td>\n</tr>\n</tbody>\n</table>\n<img src=\"@source/blog/深度学习模型.assets/image-20250405180027125.png\" alt=\"image-20250405180027125\" style=\"zoom:67%;\" />\n<p>精度/查准率：</p>\n<p><code v-pre>预测为正例的样本中实际为正例的比例</code></p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">precision = \\frac{TP}{TP+FP}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.854em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">p</span><span class=\"mord mathnormal\">rec</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">s</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\">n</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.1297em;vertical-align:-0.7693em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3603em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">TP</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">FP</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">TP</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7693em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p>召回率/查全率：</p>\n<p><code v-pre>实际为正例的样本中被正确预测的比例</code></p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">recall = \\frac{TP}{TP+FN}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">rec</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">ll</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.1297em;vertical-align:-0.7693em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3603em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">TP</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">FN</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">TP</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7693em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p><strong>mAP</strong></p>\n<ol>\n<li>划定不同阈值计算不同的精度/召回率\n<ul>\n<li>计算在不同阈值的情况下，Predicision和Recall的值。\n<ul>\n<li>阈值0.9：无视所有小于0.9的predict，那么此时TP=1,FP=0,precision=1，所有标签数目为3，那么recall=1/3</li>\n<li>阈值0.8：无视所有小于0.8的predict，那么此时TP=1,FP=1,precision=1/2，所有标签数目为3，那么recall=1/3</li>\n<li>阈值0.7：无视所有小于0.7的predict，那么此时TP=2,FP=1,precision=2/3，所有标签数目为3，那么recall=2/3</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>根据精度/召回率，绘制RP曲线，计算<strong>AP</strong>值\n<ul>\n<li>在每个”峰值点”往左画一条直线，和上一个“峰值点”的垂直线像交，这样和坐标轴框出来的面积就是AP值。</li>\n</ul>\n</li>\n</ol>\n<img src=\"@source/blog/深度学习模型.assets/image-20250405190809088.png\" alt=\"image-20250405190809088\" style=\"zoom:67%;\" />\n<ol start=\"3\">\n<li>mAP：对多个类别的检测情况评估</li>\n</ol>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>m</mi><mi>A</mi><mi>P</mi><mo>=</mo><mfrac><mrow><mo>∑</mo><mi>A</mi><mi>P</mi></mrow><mrow><mi>N</mi><mo stretchy=\"false\">(</mo><mi>c</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi><mi>e</mi><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">mAP=\\frac{\\sum AP}{N(classes)}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">m</span><span class=\"mord mathnormal\">A</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.363em;vertical-align:-0.936em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.427em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">c</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\">sses</span><span class=\"mclose\">)</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:0em;\">∑</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">A</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<h4 id=\"overfeat\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#overfeat\"><span>overfeat</span></a></h4>\n<blockquote>\n<p>时间：2013年</p>\n<p>特点：采用了一种基于 <strong>滑动窗口</strong> 和 <strong>全卷积神经网络（FCN）</strong> 的方法来实现目标的分类和定位</p>\n</blockquote>\n<img src=\"@source/blog/深度学习模型.assets/82e1d2d67195c77b6b755473adc2d542.png\" alt=\"img\" style=\"zoom:50%;\" />\n<h5 id=\"流程\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#流程\"><span>流程</span></a></h5>\n<ol>\n<li>通过FCN全卷积网络提取特征\n<ul>\n<li>首先定义若干个大小窗口（K个）</li>\n<li>K中每个窗口都要滑动图片，每个窗口都需要滑动M次</li>\n<li>得到K x M个特征图</li>\n</ul>\n</li>\n<li>对每个位置的特征图进行目标分类和定位</li>\n<li>输出每个窗口的类别得分和边框坐标</li>\n</ol>\n<h4 id=\"nms\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#nms\"><span>NMS</span></a></h4>\n<blockquote>\n<p><strong>去除冗余的候选框</strong>，只保留最具代表性的框，提升检测的准确性</p>\n</blockquote>\n<ol>\n<li>标准 NMS\n<ul>\n<li>经典的 NMS 计算方法，直接移除冗余框</li>\n<li>对于每个框，按得分降序排列，对所有其他框计算 IOU，并移除重叠框</li>\n</ul>\n</li>\n<li>Soft NMS\n<ul>\n<li>Soft NMS 不直接去除重叠的框，而是<strong>平滑</strong>它们的得分。具体来说，随着重叠度增加，框的得分会逐渐衰减（通过一个衰减因子）</li>\n<li>避免移除可能是正确框的高 IOU 框，尤其是在物体边缘的框</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"two-stage\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#two-stage\"><span>Two-Stage</span></a></h3>\n<h4 id=\"rcnn-region-based-cnn\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#rcnn-region-based-cnn\"><span>RCNN（Region-based CNN）</span></a></h4>\n<img src=\"@source/blog/深度学习模型.assets/image-20250405203801765.png\" alt=\"image-20250405203801765\" style=\"zoom:67%;\" />\n<h5 id=\"流程-1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#流程-1\"><span>流程</span></a></h5>\n<ol>\n<li>生成候选区域：基于颜色、纹理等低级特征合并超像素</li>\n<li>统一候选区域：将每个候选区域缩放至固定尺寸</li>\n<li>提取特征：使用预训练的CNN（如AlexNet）提特征</li>\n<li>分类与回归：使用SVM分类/线性回归修正边界框</li>\n</ol>\n<h4 id=\"sppnet\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#sppnet\"><span>SPPNet</span></a></h4>\n<img src=\"@source/blog/深度学习模型.assets/image-20250405204318004.png\" alt=\"image-20250405204318004\" style=\"zoom:67%;\" />\n<h5 id=\"流程-2\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#流程-2\"><span>流程</span></a></h5>\n<ol>\n<li>整图输入CNN生成特征图。</li>\n<li>候选区域通过坐标映射到特征图对应位置。</li>\n<li>使用SPP层（如4级金字塔：1x1、2x2、3x3、6x6）池化</li>\n<li>分类与回归：使用SVM分类/线性回归修正边界框</li>\n</ol>\n<h4 id=\"fast-rcnn\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#fast-rcnn\"><span>Fast RCNN</span></a></h4>\n<img src=\"@source/blog/深度学习模型.assets/image-20250405212622317.png\" alt=\"image-20250405212622317\" style=\"zoom:67%;\" />\n<h5 id=\"流程-3\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#流程-3\"><span>流程</span></a></h5>\n<ol>\n<li>整图输入CNN生成特征图</li>\n<li>用<strong>Selective Search（选择性搜索算法</strong>）生成候选框(ROIs)</li>\n<li>RoI Pooling将不同尺寸候选框(ROIs)映射到特征图并池化为统一特征图大小</li>\n<li>两个全连接层分别输出分类结果和边界框偏移量</li>\n<li>多任务损失训练分类与回归网络</li>\n</ol>\n<h4 id=\"faster-rcnn\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#faster-rcnn\"><span><mark>Faster RCNN</mark></span></a></h4>\n<img src=\"@source/blog/深度学习模型.assets/image-20250406141359665.png\" alt=\"image-20250406141359665\" style=\"zoom:67%;\" />\n<img src=\"@source/blog/深度学习模型.assets/image-20250406141705239.png\" alt=\"image-20250406141705239\" style=\"zoom: 150%;\" />\n<h5 id=\"流程-4\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#流程-4\"><span>流程</span></a></h5>\n<ol>\n<li>整图输入CNN生成特征图。</li>\n<li><strong>RPN</strong>生成候选框并过滤（NMS去除低质量候选框 + 高精度低召回率 = 量少质优~300个）</li>\n<li>RoI Pooling将候选框映射到特征图并池化</li>\n<li>两个全连接层分别输出分类结果和边界框偏移量</li>\n<li>多任务损失训练分类与回归网络</li>\n</ol>\n<h4 id=\"对比总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#对比总结\"><span>对比总结</span></a></h4>\n<img src=\"@source/blog/深度学习模型.assets/image-20250406144031898.png\" alt=\"image-20250406144031898\" style=\"zoom:67%;\" />\n<img src=\"@source/blog/深度学习模型.assets/image-20250406144052203.png\" alt=\"image-20250406144052203\" style=\"zoom:67%;\" />\n<img src=\"@source/blog/深度学习模型.assets/image-20250406144124733.png\" alt=\"image-20250406144124733\" style=\"zoom:67%;\" />\n<p><strong>R-CNN网络演进：</strong></p>\n<img src=\"@source/blog/深度学习模型.assets/image-20250406144412151.png\" alt=\"image-20250406144412151\" style=\"zoom:80%;\" />\n<h4 id=\"rfcn\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#rfcn\"><span>RFCN</span></a></h4>\n<h3 id=\"one-stage\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#one-stage\"><span>One-Stage</span></a></h3>\n<h4 id=\"ssd\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#ssd\"><span><mark>SSD</mark></span></a></h4>\n<img src=\"@source/blog/深度学习模型.assets/image-20250406162020930.png\" alt=\"image-20250406162020930\" style=\"zoom:67%;\" />\n<h5 id=\"流程-5\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#流程-5\"><span>流程</span></a></h5>\n<ol>\n<li>整图输入基础网络（如VGG）生成多尺度特征图。</li>\n<li>每个特征图位置预测K个Anchor的类别和坐标偏移。</li>\n<li>通过NMS筛选最终检测框。</li>\n</ol>\n<h4 id=\"yolov1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolov1\"><span>YOLOv1</span></a></h4>\n<p><code v-pre>输入图片：448*448*3</code></p>\n<figure><img src=\"@source/blog/深度学习模型.assets/a29c47bca5ec4f359b54fc6d313879be.png\" alt=\"a29c47bca5ec4f359b54fc6d313879be\" tabindex=\"0\" loading=\"lazy\"><figcaption>a29c47bca5ec4f359b54fc6d313879be</figcaption></figure>\n<p>每个小框预测位置信息（<code v-pre>x, y, w, h, c</code>）+  类别概率信息<code v-pre>C</code></p>\n<ul>\n<li>x：coordinate of bbox center inside cell([0;1] wrt grid cell size)</li>\n<li>y：coordinate of bbox center inside cell([0;1] wrt grid cell size)</li>\n<li>w：bbox width ([0;1] wrt image)</li>\n<li>h：bbox width ([0;1] wrt image)</li>\n<li>c：bbox confidence ~ P(obj in bbox1)</li>\n<li>C：C个不同类别概率信息</li>\n</ul>\n<h4 id=\"yolov2\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolov2\"><span>YOLOv2</span></a></h4>\n<h4 id=\"yolov3\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolov3\"><span>YOLOv3</span></a></h4>\n<h4 id=\"yolov4\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolov4\"><span>YOLOv4</span></a></h4>\n<h4 id=\"yolov5\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolov5\"><span>YOLOv5</span></a></h4>\n<figure><img src=\"@source/blog/深度学习模型.assets/image-20250413193305078.png\" alt=\"image-20250413193305078\" tabindex=\"0\" loading=\"lazy\"><figcaption>image-20250413193305078</figcaption></figure>\n<figure><img src=\"@source/blog/深度学习模型.assets/image-20250413190728754.png\" alt=\"image-20250413190728754\" tabindex=\"0\" loading=\"lazy\"><figcaption>image-20250413190728754</figcaption></figure>\n<figure><img src=\"@source/blog/深度学习模型.assets/image-20250413190742275.png\" alt=\"image-20250413190742275\" tabindex=\"0\" loading=\"lazy\"><figcaption>image-20250413190742275</figcaption></figure>\n<h4 id=\"yolov6\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolov6\"><span>YOLOv6</span></a></h4>\n<h4 id=\"yolov7\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolov7\"><span>YOLOv7</span></a></h4>\n<h4 id=\"yolov8\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolov8\"><span>YOLOv8</span></a></h4>\n<h4 id=\"yolov12\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolov12\"><span>YOLOv12</span></a></h4>\n<h2 id=\"大模型与多模态\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#大模型与多模态\"><span>大模型与多模态</span></a></h2>\n<h3 id=\"概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#概述\"><span>概述</span></a></h3>\n<p><code v-pre>基本Pipeline：问题明确-&gt;数据获取-&gt;数据清理-&gt;数据探索-&gt;数据准备-&gt;训练模型-&gt;微调模型-&gt;结果应用-&gt;监控迭代</code></p>\n<h4 id=\"整体视角\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#整体视角\"><span>整体视角</span></a></h4>\n<ul>\n<li>数据决定算法的上线，模型只是去逼近这个上线</li>\n<li>算法工程师的基础能力：数据采集、评估、传输、预处理、标注、分析、挖掘、特征融合等</li>\n</ul>\n<h4 id=\"llm构建流程\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#llm构建流程\"><span>LLM构建流程</span></a></h4>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>预训练</th>\n<th>有监督微调</th>\n<th>奖励建模</th>\n<th>强化学习</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>数据集合</td>\n<td>原始数据<br />[<mark>数千亿</mark>单词：图书、百科、网页等]</td>\n<td>标注用户指令<br />[<mark>数万</mark>用户指令和对应的答案]</td>\n<td>标注对比对<br />[<mark>数万量级</mark>标注对比对]</td>\n<td>用户指令<br />[<mark>十万量级</mark>用户指令]</td>\n</tr>\n<tr>\n<td>算法</td>\n<td>语言模型训练</td>\n<td>语言模型训练</td>\n<td>二分类模型</td>\n<td>强化学习方法</td>\n</tr>\n<tr>\n<td>模型</td>\n<td>基础模型</td>\n<td>SFT模型</td>\n<td>RM模型</td>\n<td>RL模型</td>\n</tr>\n<tr>\n<td>资源需求</td>\n<td>1000+GPU[月]</td>\n<td>1-100GPU[天]</td>\n<td>1-100GPU[天]</td>\n<td>1-100GPU[天]</td>\n</tr>\n</tbody>\n</table>\n<p><strong>有监督微调：</strong></p>\n<p><code v-pre>指令微调(Instruction Tuning)利用少量高质量数据集合，包含用户输入的提示词(Prompt)和对应的理想输出结果。用户输入包括问题、闲聊对话、任务指令等多种形式和任务</code></p>\n<ul>\n<li>如何微调？利用高质量有监督数据，使用与训练阶段相同的语言模型训练算法，在基础语言模型基础上再训练，得到有监督微调模型(SFT模型)</li>\n<li>微调后的效果：具备初步指令理解能力和上下文理解能力，能够完成开放领域问题、阅读理解、翻译、生成代码等能力，也具备一定对未知任务的泛化能力</li>\n</ul>\n<p><strong>下游任务微调：</strong></p>\n<p><code v-pre>DownstreamTaskFine-tuning</code></p>\n<p>目的：在通用语义表示基础上，根据下游任务的特性进行适配</p>\n<p>注意：容易使得模型遗忘预训练阶段学习到的通用语义知识表示，损失模型的通用性和泛化能力，造成灾难性遗忘(CatastrophicForgetting)问题，因此通常采用混合预训练任务损失和下游微调损失的方法来缓解</p>\n<p><strong>奖励建模：</strong></p>\n<p><code v-pre>Reward Modeling</code></p>\n<p>目的：构建一个文本质量对比模型，对于同一个提示词，SFT模型给出的多个不同输出结果的质量进行排序</p>\n<p>注意：RM模型的准确率对于强化学习阶段的效果有至关重要的影响，因此需要大规模训练数据</p>\n<p><strong>强化学习：</strong></p>\n<p><code v-pre>Reinforcement Learning</code></p>\n<p>流程：根据数十万用户给出的提示词，利用在前一阶段训练的RM模型，给出CFT模型对用户提示词补全结果的质量评估，并与语言模型建模目标综合得到更好效果</p>\n<p>该阶段使得基础模型的熵降低，会减少模型输出的多样性</p>\n<ol>\n<li>从数据集中sample一个prompt</li>\n<li>语言模型(policy)生成输出</li>\n<li>使用奖励模型(Environment)计算得分<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>r</mi><mi>θ</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">r\\theta(x,y)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span></span>(Reward)由<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>r</mi><mi>θ</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">r\\theta(x,y)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span></span>使用PPO-ptx算法优化语言模型</li>\n</ol>\n<img src=\"@source/blog/深度学习模型.assets/image-20250418215738655.png\" alt=\"image-20250418215738655\" style=\"zoom: 80%;\" />\n<h4 id=\"llm参数\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#llm参数\"><span>LLM参数</span></a></h4>\n<h5 id=\"采样系数top-k\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#采样系数top-k\"><span>采样系数Top-k</span></a></h5>\n<p><code v-pre>如何预测下一个词</code></p>\n<blockquote>\n<p>在某一解码时间步，固定选取前k个概率对应的词作为候选，并按照概率进行采样</p>\n<p>采样并不代表每次都会选概率最大的，只是概率越大被选中的几率越大</p>\n</blockquote>\n<img src=\"@source/blog/深度学习模型.assets/image-20250419143534028.png\" alt=\"image-20250419143534028\" style=\"zoom: 67%;\" />\n<p><strong>top-k值对解码效果影响：</strong></p>\n<ul>\n<li>k值变大：选择范围变大，输出更加多样化但精确度也会降低</li>\n<li>k值变小：输出更加确定但缺乏多样性</li>\n</ul>\n<p><strong>缺点：</strong></p>\n<ul>\n<li>不会更具词的概率分布动态调整k值</li>\n</ul>\n<h5 id=\"采样系数top-p\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#采样系数top-p\"><span>采样系数Top-p</span></a></h5>\n<blockquote>\n<p>解决了Top-k采样中只能固定选取前k个词的问题</p>\n<p>在某一解码时间步，动态选取概率之和大于p的最小集合作为候选，并按照概率进行采样</p>\n</blockquote>\n<img src=\"@source/blog/深度学习模型.assets/image-20250419143557550.png\" alt=\"image-20250419143557550\" style=\"zoom:67%;\" />\n<p><strong>给定p值时，候选词列表的大小主要由概率分布决定：</strong></p>\n<ul>\n<li>如果模型对下一个词比较确定，则候选词列表会比较小</li>\n<li>反之，概率分布会相对均匀(对下一个词不确定)，此时候选列表会相对大一些</li>\n</ul>\n<p><strong>实际应用：</strong></p>\n<ul>\n<li>将<code v-pre>Top-k</code>和<code v-pre>Top-p</code>方法进行结合，先应用<code v-pre>Top-k</code>，然后应用<code v-pre>Top-p</code></li>\n</ul>\n<h5 id=\"温度系数temperature\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#温度系数temperature\"><span>温度系数Temperature</span></a></h5>\n<p><code v-pre>控制了softmax输出分布，Temperature=1时退化为标准softmax函数</code></p>\n<figure><img src=\"@source/blog/深度学习模型.assets/image-20250419144841787.png\" alt=\"image-20250419144841787\" tabindex=\"0\" loading=\"lazy\"><figcaption>image-20250419144841787</figcaption></figure>\n<p><strong>Temperature对输出结果的影响：</strong></p>\n<ul>\n<li>当Temperature较低时(如0.1/0.2)：模型倾向于选择概率较高的单词，生成的文本较为连贯和准确，但可能显得过于保守，缺乏创造性和多样性</li>\n<li>当Temperature较高时(如0.8/1.0)：模型倾向于选择概率较低的单词，生成的文本较为多样和创造，但可能牺牲了一定的连贯性和准确性</li>\n</ul>\n<p><strong>应用技巧：</strong></p>\n<ul>\n<li>LLM中普遍取值一般为0.2~1.0</li>\n<li>对于多样性要求较高的任务(例如对话、文本生成)可适当提高温度系数</li>\n</ul>\n<h4 id=\"预训练模型分类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#预训练模型分类\"><span>预训练模型分类</span></a></h4>\n<h5 id=\"nlu类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#nlu类\"><span>NLU类</span></a></h5>\n<p><code v-pre>自然语言理解</code></p>\n<ul>\n<li>以<code v-pre>BERT</code>为代表的自编码预训练模型，NLU任务：分配、抽取等</li>\n<li>如何训练？借助特定的预训练任务进行学习，如：掩码语言模型(MLM)、下一个句子预测(NSP)等</li>\n<li>双向语言模型，同时建模上文和下文信息</li>\n<li>代表模型：RoBERTa、ALBERT、ELECTRA、DeBERTa等</li>\n<li>NLU任务特点：输出范围确定、评价方法相对明确</li>\n</ul>\n<h5 id=\"nlg类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#nlg类\"><span>NLG类</span></a></h5>\n<p><code v-pre>自然语言生成</code></p>\n<ul>\n<li>以<code v-pre>GPT</code>为代表的自回归预训练模型，NLG任务：文本生成、生成式摘要、对话等</li>\n<li>如何训练？使用Causal LM训练(N-gram语言模型的自然延申)，无需设计复杂的预训练任务</li>\n<li>单向语言模型，部分模型采用双向编码器和单向编码器结构</li>\n<li>代表模型：GPT系列(Decoder-only)、T5和BART(Encoder-Decoder)等</li>\n<li>NLG任务特点：输出自由度搞、评价方法较难、更具有创造性</li>\n</ul>\n<h4 id=\"模型的涌现能力\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型的涌现能力\"><span>模型的涌现能力</span></a></h4>\n<p><code v-pre>《Emergent Abilities of Large Language Models》</code></p>\n<h5 id=\"基于模型放大\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#基于模型放大\"><span>基于模型放大</span></a></h5>\n<ul>\n<li>TruthfulQA：当模型放大至280B，其效果会突然高于随机20%</li>\n<li>Multi-task language understanding：当训练计算量达到70B-280B后效果将远远超过随机</li>\n<li>Word in Context：当PaLM被缩放至540B时，高于随机的效果出现</li>\n</ul>\n<p>根据文章：<code v-pre>Scaling Laws</code> for Neural Language Models</p>\n<img src=\"@source/blog/深度学习模型.assets/image-20250418213352077.png\" alt=\"image-20250418213352077\" style=\"zoom:67%;\" />\n<h5 id=\"基于样例提示\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#基于样例提示\"><span>基于样例提示</span></a></h5>\n<p>通过 few-shot prompting来执行任务的能力也是一种涌现现象</p>\n<h3 id=\"transformer\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#transformer\"><span>transformer</span></a></h3>\n<figure><img src=\"@source/blog/深度学习模型.assets/3319e3d6922a2e7f2499a3130d3b5925.png\" alt=\"在这里插入图片描述\" tabindex=\"0\" loading=\"lazy\"><figcaption>在这里插入图片描述</figcaption></figure>\n<ul>\n<li>BERT（Bidirectional Encoder Representations from Transformers）：双向语言理解模型，仅使用 编码器，用于理解整个句子的上下文，适合分类、问答等理解类任务。</li>\n<li>GPT（Generative Pre-trained Transformer）：自回归语言模型，仅使用 解码器，其设计目的是生成下一个词，适合用于生成式任务，如文本生成、对话等。</li>\n</ul>\n<h3 id=\"多模态\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#多模态\"><span>多模态</span></a></h3>\n<h4 id=\"基本结构\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#基本结构\"><span>基本结构</span></a></h4>\n<h5 id=\"vit\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#vit\"><span>vit</span></a></h5>\n<img src=\"@source/blog/深度学习模型.assets/image-20250504143710226.png\" alt=\"image-20250504143710226\" style=\"zoom:67%;\" />\n<h5 id=\"yolos\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolos\"><span>yolos</span></a></h5>\n<img src=\"@source/blog/深度学习模型.assets/image-20250504143729206.png\" alt=\"image-20250504143729206\" style=\"zoom: 80%;\" />\n<h4 id=\"图文匹配\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#图文匹配\"><span>图文匹配</span></a></h4>\n<h5 id=\"clip\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#clip\"><span>clip</span></a></h5>\n<img src=\"@source/blog/深度学习模型.assets/image-20250504143559792.png\" alt=\"image-20250504143559792\" style=\"zoom:67%;\" />\n<img src=\"@source/blog/深度学习模型.assets/image-20250504150014695.png\" alt=\"image-20250504150014695\" style=\"zoom:80%;\" />\n<figure><img src=\"@source/blog/深度学习模型.assets/image-20250504150103155.png\" alt=\"image-20250504150103155\" tabindex=\"0\" loading=\"lazy\"><figcaption>image-20250504150103155</figcaption></figure>\n<h5 id=\"bridge-tower\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#bridge-tower\"><span>bridge tower</span></a></h5>\n<img src=\"@source/blog/深度学习模型.assets/image-20250504160101916.png\" alt=\"image-20250504160101916\" style=\"zoom:80%;\" />\n<img src=\"@source/blog/深度学习模型.assets/image-20250504160137868.png\" alt=\"image-20250504160137868\" style=\"zoom: 67%;\" />\n<h5 id=\"gpt4原理\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#gpt4原理\"><span>gpt4原理</span></a></h5>\n<img src=\"@source/blog/深度学习模型.assets/image-20250504160229505.png\" alt=\"image-20250504160229505\" style=\"zoom:67%;\" />\n<h4 id=\"文生图-图生文\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#文生图-图生文\"><span>文生图/图生文</span></a></h4>\n<h5 id=\"dall-·-e\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#dall-·-e\"><span>DALL · E</span></a></h5>\n<img src=\"@source/blog/深度学习模型.assets/image-20250504154828197.png\" alt=\"image-20250504154828197\" style=\"zoom:80%;\" />\n<img src=\"@source/blog/深度学习模型.assets/image-20250504154919514.png\" alt=\"image-20250504154919514\" style=\"zoom:80%;\" />\n<h4 id=\"扩散模型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#扩散模型\"><span>扩散模型</span></a></h4>\n<h2 id=\"模型微调\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型微调\"><span>模型微调</span></a></h2>\n<p><code v-pre>处理不当，很可能造成模型原始能力的灾难性以往、即回导致模型原始能力丢失，对于复杂模型更是如此</code></p>\n<h3 id=\"微调概念\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#微调概念\"><span>微调概念</span></a></h3>\n<h4 id=\"全量微调\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#全量微调\"><span>全量微调</span></a></h4>\n<ul>\n<li>对所有参数进行微调</li>\n<li>对算力和显存要求高</li>\n<li>效果最佳</li>\n</ul>\n<h4 id=\"局部微调\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#局部微调\"><span>局部微调</span></a></h4>\n<blockquote>\n<p>重要调整输入输出层效果明显，而非中间层</p>\n</blockquote>\n<ul>\n<li>只调整某些<strong>某部分参数</strong>，例如输入层，输出层或某些特殊层</li>\n<li>对算力和显存要求一般</li>\n<li>一定是有效的</li>\n</ul>\n<h4 id=\"增量微调\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#增量微调\"><span>增量微调</span></a></h4>\n<blockquote>\n<p>前/后增量：</p>\n<ol>\n<li>改变任务（如：2-&gt;8分类），选择后增</li>\n<li>模型是一个提取特征的过程，后增效果好</li>\n</ol>\n</blockquote>\n<ul>\n<li>通过<strong>新增参数</strong>的方式进行微调，新的知识存储在新的参数中。</li>\n<li>对显存和算力要求低</li>\n<li>效果不如全量微调</li>\n</ul>\n<h3 id=\"微调方式\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#微调方式\"><span>微调方式</span></a></h3>\n<h4 id=\"lora-low-rank-adaption\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#lora-low-rank-adaption\"><span>LoRA（Low-Rank Adaption）</span></a></h4>\n<p>通过引入低秩矩阵来减少微调时的参数量。在预训练的模型中，LoRA通过添加两个小矩阵A和B来近似原始的大矩阵<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">Δ</mi><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Δ</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span></span></span>，从而减少需要更新的参数数量。</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>W</mi><mn>0</mn></msub><mo>+</mo><mi mathvariant=\"normal\">Δ</mi><mi>W</mi><mo>=</mo><msub><mi>W</mi><mn>0</mn></msub><mo>+</mo><mi>B</mi><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">W_0+\\Delta W=W_0+BA\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Δ</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B</span><span class=\"mord mathnormal\">A</span></span></span></span></span></p>\n<ul>\n<li>A和B的秩远小于原始矩阵的秩，从而大大减少了需要更新的参数数量</li>\n</ul>\n<figure><img src=\"@source/blog/深度学习模型.assets/image-20250616215925193.png\" alt=\"image-20250616215925193\" tabindex=\"0\" loading=\"lazy\"><figcaption>image-20250616215925193</figcaption></figure>\n<ul>\n<li>训练时：输入分别与原始权重和两个低秩矩阵进行计算，共同的到最终结果，优化则仅优化A和B</li>\n<li>训练完成后，可以将两个低秩矩阵与原始模型中的权重进行合并，合并后的模型与原始模型无异</li>\n</ul>\n<h4 id=\"qlora\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#qlora\"><span>QLoRA</span></a></h4>\n<p><code v-pre>Quantized Low-Rank Adaption，在LoRA的基础上加入量化技术，减少权重表示的位数，从而降低显存和计算需求</code></p>\n<ul>\n<li>量化：将模型权重量化为低精度（如INT4），减少内存占用，并提高推理和训练速度</li>\n</ul>\n<h3 id=\"微调工具\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#微调工具\"><span>微调工具</span></a></h3>\n<h4 id=\"llama-factory\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#llama-factory\"><span>Llama-Factory</span></a></h4>\n<h4 id=\"xtuner\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#xtuner\"><span>Xtuner</span></a></h4>\n<h3 id=\"性能评估-evalscope\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#性能评估-evalscope\"><span>性能评估：EvalScope</span></a></h3>\n<p>https://evalscope.readthedocs.io/zh-cn/latest/best_practice/qwen3.html#id7</p>\n<h2 id=\"模型部署\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型部署\"><span>模型部署</span></a></h2>\n<h3 id=\"ollama\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#ollama\"><span>Ollama</span></a></h3>\n<blockquote>\n<ul>\n<li>针对个人用户</li>\n<li>必须是gguf格式的模型【GGUF格式一般是量化后的】</li>\n</ul>\n</blockquote>\n<h4 id=\"安装\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#安装\"><span>安装</span></a></h4>\n<p><strong>一键安装</strong></p>\n<div class=\"language-bash line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"bash\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code class=\"language-bash\"><span class=\"line\"><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">curl</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> -fsSL</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> https://ollama.com/install.sh</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> |</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\"> sh</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">sudo</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> vim</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> /etc/systemd/system/ollama.service</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">sudo</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> ufw</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> allow</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> 11434/tcp</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">sudo</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> systemctl</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> daemon-reload</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> &#x26;</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\"> sudo</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> systemctl</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> restart</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> ollama</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><p><strong>ollama.service</strong></p>\n<p><code v-pre>使用显卡进行推理</code></p>\n<div class=\"language-toml line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"toml\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code class=\"language-toml\"><span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">Unit</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">Description</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">O</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">llama Service</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">After</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">n</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">etwork-online.target</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">Service</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">ExecStart</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">/</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">usr/local/bin/ollama serve</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">User</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">o</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">llama</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">Group</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">o</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">llama</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">Restart</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">a</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">lways</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">RestartSec</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">3</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">Environment</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">PATH=/mnt/big_disk_0/spa/.cargo/bin:/mnt/big_disk_0/spa/miniconda3/bin:/mnt/big_disk_0/spa/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/mnt/big_disk_0/spa/.local/bin:/mnt/big_disk_0/spa/bin</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">Environment</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">OLLAMA_HOST=0.0.0.0:11434</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">Environment</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">CUDA_VISIBLE_DEVICES=1,0</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">Install</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">WantedBy</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">d</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">efault.target</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><h4 id=\"使用\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#使用\"><span>使用</span></a></h4>\n<div class=\"language-bash line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"bash\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code class=\"language-bash\"><span class=\"line\"><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">ollama</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> serve</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div></div></div><div class=\"language-bash line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"bash\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code class=\"language-bash\"><span class=\"line\"><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">ollama</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> run</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> 模型:参数B</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div></div></div><h3 id=\"vllm\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#vllm\"><span>Vllm</span></a></h3>\n<blockquote>\n<ul>\n<li>企业专用</li>\n<li>一次只支持部署一个模型</li>\n</ul>\n</blockquote>\n<div class=\"language-bash line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"bash\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code class=\"language-bash\"><span class=\"line\"><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">vllm</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> serve</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> /mnt/big_disk/big_disk_3/spa/Code/Study/llm/XiaomiMiMo/MiMo-7B-RL</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> \\</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">\t--trust_remote_code</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> \\</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">    --tensor-parallel-size</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 4</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> \\</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">    --served-model-name</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> MiMo-7B-RL</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><h3 id=\"lmdeploy\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#lmdeploy\"><span>LMDeploy</span></a></h3>\n<blockquote>\n<ul>\n<li>显存优化比vllm好点</li>\n<li>只支持列表内模型</li>\n</ul>\n</blockquote>\n<div class=\"language-bash line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"bash\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code class=\"language-bash\"><span class=\"line\"><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">lmdeploy</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> serve</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> api_server</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> internlm/internlm2_5-7b-chat</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div></div></div>","env":{"base":"/","filePath":"/Users/hxhy/Code/Blog/docs/blog/深度学习模型.md","filePathRelative":"blog/深度学习模型.md","frontmatter":{"title":"模型","createTime":"2025/03/24 09:26:50","permalink":"/article/x3feqx82/"},"sfcBlocks":{"template":{"type":"template","content":"<template><h2 id=\"视觉模型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#视觉模型\"><span>视觉模型</span></a></h2>\n<h3 id=\"分类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#分类\"><span>分类</span></a></h3>\n<h4 id=\"任务\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#任务\"><span>任务</span></a></h4>\n<ul>\n<li>\n<p>图像分类：识别出图中出现的物体类别是什么，其功能主要是用于判断是什么？</p>\n<ul>\n<li>VGG</li>\n<li>GoogleNet</li>\n<li>ResNet</li>\n</ul>\n</li>\n<li>\n<p>图像定位：不仅仅需要识别出是什么物体（即分类）同时需要预测物体的位置信息，也就是单个目标在哪里？是什么？</p>\n<ul>\n<li>RCNN</li>\n<li>Fast RCNN</li>\n<li>Faster RCNN</li>\n</ul>\n</li>\n<li>\n<p>目标检测：多目标的定位，即在一个图片中定位多个目标物体，包括分类和定位，也就是多个目标分别在哪里？分别属于那个类别？</p>\n<ul>\n<li>RCNN</li>\n<li>Fast RNN</li>\n<li>Faster RCNN</li>\n<li>SSD</li>\n<li>YOLO</li>\n</ul>\n</li>\n</ul>\n<img src=\"@source/blog/深度学习模型.assets/image-20250405172047846.png\" alt=\"image-20250405172047846\" style=\"zoom:80%;\" />\n<h4 id=\"模型架构\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型架构\"><span>模型架构</span></a></h4>\n<ul>\n<li>二阶段：R-CNN、Fast R-CNN、Faster-R-CNN、SPP-Net、R-FCN</li>\n<li>一阶段：YOLO、SSD、FPN</li>\n</ul>\n<p>图像分割与目标检测：Cascade R-CNN</p>\n<h3 id=\"参数介绍\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#参数介绍\"><span>参数介绍</span></a></h3>\n<h4 id=\"iou\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#iou\"><span>IOU</span></a></h4>\n<p><code v-pre>两个边界框(bounding box)的重叠度</code></p>\n<img src=\"@source/blog/深度学习模型.assets/image-20250405175924671.png\" alt=\"image-20250405175924671\" style=\"zoom:50%;\" />\n$$\nIOU=\\frac{A\\cap B}{A\\cup B}=\\frac{S_{_{A,B}}}{S_{_A}+S_{_B}-S_{_{A,B}}}\n$$\n<h4 id=\"map\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#map\"><span>MAP</span></a></h4>\n<p><strong>精度和召回率</strong></p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>正例（实际）</th>\n<th>负例（实际）</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>正例（预测）</strong></td>\n<td>TP</td>\n<td>FP</td>\n</tr>\n<tr>\n<td><strong>负例（预测）</strong></td>\n<td>FN</td>\n<td>TN</td>\n</tr>\n</tbody>\n</table>\n<img src=\"@source/blog/深度学习模型.assets/image-20250405180027125.png\" alt=\"image-20250405180027125\" style=\"zoom:67%;\" />\n<p>精度/查准率：</p>\n<p><code v-pre>预测为正例的样本中实际为正例的比例</code></p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">precision = \\frac{TP}{TP+FP}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.854em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">p</span><span class=\"mord mathnormal\">rec</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">s</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\">n</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.1297em;vertical-align:-0.7693em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3603em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">TP</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">FP</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">TP</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7693em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p>召回率/查全率：</p>\n<p><code v-pre>实际为正例的样本中被正确预测的比例</code></p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">recall = \\frac{TP}{TP+FN}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">rec</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">ll</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.1297em;vertical-align:-0.7693em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3603em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">TP</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">FN</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">TP</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7693em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p><strong>mAP</strong></p>\n<ol>\n<li>划定不同阈值计算不同的精度/召回率\n<ul>\n<li>计算在不同阈值的情况下，Predicision和Recall的值。\n<ul>\n<li>阈值0.9：无视所有小于0.9的predict，那么此时TP=1,FP=0,precision=1，所有标签数目为3，那么recall=1/3</li>\n<li>阈值0.8：无视所有小于0.8的predict，那么此时TP=1,FP=1,precision=1/2，所有标签数目为3，那么recall=1/3</li>\n<li>阈值0.7：无视所有小于0.7的predict，那么此时TP=2,FP=1,precision=2/3，所有标签数目为3，那么recall=2/3</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>根据精度/召回率，绘制RP曲线，计算<strong>AP</strong>值\n<ul>\n<li>在每个”峰值点”往左画一条直线，和上一个“峰值点”的垂直线像交，这样和坐标轴框出来的面积就是AP值。</li>\n</ul>\n</li>\n</ol>\n<img src=\"@source/blog/深度学习模型.assets/image-20250405190809088.png\" alt=\"image-20250405190809088\" style=\"zoom:67%;\" />\n<ol start=\"3\">\n<li>mAP：对多个类别的检测情况评估</li>\n</ol>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>m</mi><mi>A</mi><mi>P</mi><mo>=</mo><mfrac><mrow><mo>∑</mo><mi>A</mi><mi>P</mi></mrow><mrow><mi>N</mi><mo stretchy=\"false\">(</mo><mi>c</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi><mi>e</mi><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">mAP=\\frac{\\sum AP}{N(classes)}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">m</span><span class=\"mord mathnormal\">A</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.363em;vertical-align:-0.936em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.427em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">c</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\">sses</span><span class=\"mclose\">)</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:0em;\">∑</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">A</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<h4 id=\"overfeat\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#overfeat\"><span>overfeat</span></a></h4>\n<blockquote>\n<p>时间：2013年</p>\n<p>特点：采用了一种基于 <strong>滑动窗口</strong> 和 <strong>全卷积神经网络（FCN）</strong> 的方法来实现目标的分类和定位</p>\n</blockquote>\n<img src=\"@source/blog/深度学习模型.assets/82e1d2d67195c77b6b755473adc2d542.png\" alt=\"img\" style=\"zoom:50%;\" />\n<h5 id=\"流程\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#流程\"><span>流程</span></a></h5>\n<ol>\n<li>通过FCN全卷积网络提取特征\n<ul>\n<li>首先定义若干个大小窗口（K个）</li>\n<li>K中每个窗口都要滑动图片，每个窗口都需要滑动M次</li>\n<li>得到K x M个特征图</li>\n</ul>\n</li>\n<li>对每个位置的特征图进行目标分类和定位</li>\n<li>输出每个窗口的类别得分和边框坐标</li>\n</ol>\n<h4 id=\"nms\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#nms\"><span>NMS</span></a></h4>\n<blockquote>\n<p><strong>去除冗余的候选框</strong>，只保留最具代表性的框，提升检测的准确性</p>\n</blockquote>\n<ol>\n<li>标准 NMS\n<ul>\n<li>经典的 NMS 计算方法，直接移除冗余框</li>\n<li>对于每个框，按得分降序排列，对所有其他框计算 IOU，并移除重叠框</li>\n</ul>\n</li>\n<li>Soft NMS\n<ul>\n<li>Soft NMS 不直接去除重叠的框，而是<strong>平滑</strong>它们的得分。具体来说，随着重叠度增加，框的得分会逐渐衰减（通过一个衰减因子）</li>\n<li>避免移除可能是正确框的高 IOU 框，尤其是在物体边缘的框</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"two-stage\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#two-stage\"><span>Two-Stage</span></a></h3>\n<h4 id=\"rcnn-region-based-cnn\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#rcnn-region-based-cnn\"><span>RCNN（Region-based CNN）</span></a></h4>\n<img src=\"@source/blog/深度学习模型.assets/image-20250405203801765.png\" alt=\"image-20250405203801765\" style=\"zoom:67%;\" />\n<h5 id=\"流程-1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#流程-1\"><span>流程</span></a></h5>\n<ol>\n<li>生成候选区域：基于颜色、纹理等低级特征合并超像素</li>\n<li>统一候选区域：将每个候选区域缩放至固定尺寸</li>\n<li>提取特征：使用预训练的CNN（如AlexNet）提特征</li>\n<li>分类与回归：使用SVM分类/线性回归修正边界框</li>\n</ol>\n<h4 id=\"sppnet\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#sppnet\"><span>SPPNet</span></a></h4>\n<img src=\"@source/blog/深度学习模型.assets/image-20250405204318004.png\" alt=\"image-20250405204318004\" style=\"zoom:67%;\" />\n<h5 id=\"流程-2\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#流程-2\"><span>流程</span></a></h5>\n<ol>\n<li>整图输入CNN生成特征图。</li>\n<li>候选区域通过坐标映射到特征图对应位置。</li>\n<li>使用SPP层（如4级金字塔：1x1、2x2、3x3、6x6）池化</li>\n<li>分类与回归：使用SVM分类/线性回归修正边界框</li>\n</ol>\n<h4 id=\"fast-rcnn\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#fast-rcnn\"><span>Fast RCNN</span></a></h4>\n<img src=\"@source/blog/深度学习模型.assets/image-20250405212622317.png\" alt=\"image-20250405212622317\" style=\"zoom:67%;\" />\n<h5 id=\"流程-3\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#流程-3\"><span>流程</span></a></h5>\n<ol>\n<li>整图输入CNN生成特征图</li>\n<li>用<strong>Selective Search（选择性搜索算法</strong>）生成候选框(ROIs)</li>\n<li>RoI Pooling将不同尺寸候选框(ROIs)映射到特征图并池化为统一特征图大小</li>\n<li>两个全连接层分别输出分类结果和边界框偏移量</li>\n<li>多任务损失训练分类与回归网络</li>\n</ol>\n<h4 id=\"faster-rcnn\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#faster-rcnn\"><span><mark>Faster RCNN</mark></span></a></h4>\n<img src=\"@source/blog/深度学习模型.assets/image-20250406141359665.png\" alt=\"image-20250406141359665\" style=\"zoom:67%;\" />\n<img src=\"@source/blog/深度学习模型.assets/image-20250406141705239.png\" alt=\"image-20250406141705239\" style=\"zoom: 150%;\" />\n<h5 id=\"流程-4\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#流程-4\"><span>流程</span></a></h5>\n<ol>\n<li>整图输入CNN生成特征图。</li>\n<li><strong>RPN</strong>生成候选框并过滤（NMS去除低质量候选框 + 高精度低召回率 = 量少质优~300个）</li>\n<li>RoI Pooling将候选框映射到特征图并池化</li>\n<li>两个全连接层分别输出分类结果和边界框偏移量</li>\n<li>多任务损失训练分类与回归网络</li>\n</ol>\n<h4 id=\"对比总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#对比总结\"><span>对比总结</span></a></h4>\n<img src=\"@source/blog/深度学习模型.assets/image-20250406144031898.png\" alt=\"image-20250406144031898\" style=\"zoom:67%;\" />\n<img src=\"@source/blog/深度学习模型.assets/image-20250406144052203.png\" alt=\"image-20250406144052203\" style=\"zoom:67%;\" />\n<img src=\"@source/blog/深度学习模型.assets/image-20250406144124733.png\" alt=\"image-20250406144124733\" style=\"zoom:67%;\" />\n<p><strong>R-CNN网络演进：</strong></p>\n<img src=\"@source/blog/深度学习模型.assets/image-20250406144412151.png\" alt=\"image-20250406144412151\" style=\"zoom:80%;\" />\n<h4 id=\"rfcn\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#rfcn\"><span>RFCN</span></a></h4>\n<h3 id=\"one-stage\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#one-stage\"><span>One-Stage</span></a></h3>\n<h4 id=\"ssd\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#ssd\"><span><mark>SSD</mark></span></a></h4>\n<img src=\"@source/blog/深度学习模型.assets/image-20250406162020930.png\" alt=\"image-20250406162020930\" style=\"zoom:67%;\" />\n<h5 id=\"流程-5\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#流程-5\"><span>流程</span></a></h5>\n<ol>\n<li>整图输入基础网络（如VGG）生成多尺度特征图。</li>\n<li>每个特征图位置预测K个Anchor的类别和坐标偏移。</li>\n<li>通过NMS筛选最终检测框。</li>\n</ol>\n<h4 id=\"yolov1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolov1\"><span>YOLOv1</span></a></h4>\n<p><code v-pre>输入图片：448*448*3</code></p>\n<figure><img src=\"@source/blog/深度学习模型.assets/a29c47bca5ec4f359b54fc6d313879be.png\" alt=\"a29c47bca5ec4f359b54fc6d313879be\" tabindex=\"0\" loading=\"lazy\"><figcaption>a29c47bca5ec4f359b54fc6d313879be</figcaption></figure>\n<p>每个小框预测位置信息（<code v-pre>x, y, w, h, c</code>）+  类别概率信息<code v-pre>C</code></p>\n<ul>\n<li>x：coordinate of bbox center inside cell([0;1] wrt grid cell size)</li>\n<li>y：coordinate of bbox center inside cell([0;1] wrt grid cell size)</li>\n<li>w：bbox width ([0;1] wrt image)</li>\n<li>h：bbox width ([0;1] wrt image)</li>\n<li>c：bbox confidence ~ P(obj in bbox1)</li>\n<li>C：C个不同类别概率信息</li>\n</ul>\n<h4 id=\"yolov2\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolov2\"><span>YOLOv2</span></a></h4>\n<h4 id=\"yolov3\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolov3\"><span>YOLOv3</span></a></h4>\n<h4 id=\"yolov4\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolov4\"><span>YOLOv4</span></a></h4>\n<h4 id=\"yolov5\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolov5\"><span>YOLOv5</span></a></h4>\n<figure><img src=\"@source/blog/深度学习模型.assets/image-20250413193305078.png\" alt=\"image-20250413193305078\" tabindex=\"0\" loading=\"lazy\"><figcaption>image-20250413193305078</figcaption></figure>\n<figure><img src=\"@source/blog/深度学习模型.assets/image-20250413190728754.png\" alt=\"image-20250413190728754\" tabindex=\"0\" loading=\"lazy\"><figcaption>image-20250413190728754</figcaption></figure>\n<figure><img src=\"@source/blog/深度学习模型.assets/image-20250413190742275.png\" alt=\"image-20250413190742275\" tabindex=\"0\" loading=\"lazy\"><figcaption>image-20250413190742275</figcaption></figure>\n<h4 id=\"yolov6\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolov6\"><span>YOLOv6</span></a></h4>\n<h4 id=\"yolov7\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolov7\"><span>YOLOv7</span></a></h4>\n<h4 id=\"yolov8\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolov8\"><span>YOLOv8</span></a></h4>\n<h4 id=\"yolov12\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolov12\"><span>YOLOv12</span></a></h4>\n<h2 id=\"大模型与多模态\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#大模型与多模态\"><span>大模型与多模态</span></a></h2>\n<h3 id=\"概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#概述\"><span>概述</span></a></h3>\n<p><code v-pre>基本Pipeline：问题明确-&gt;数据获取-&gt;数据清理-&gt;数据探索-&gt;数据准备-&gt;训练模型-&gt;微调模型-&gt;结果应用-&gt;监控迭代</code></p>\n<h4 id=\"整体视角\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#整体视角\"><span>整体视角</span></a></h4>\n<ul>\n<li>数据决定算法的上线，模型只是去逼近这个上线</li>\n<li>算法工程师的基础能力：数据采集、评估、传输、预处理、标注、分析、挖掘、特征融合等</li>\n</ul>\n<h4 id=\"llm构建流程\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#llm构建流程\"><span>LLM构建流程</span></a></h4>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>预训练</th>\n<th>有监督微调</th>\n<th>奖励建模</th>\n<th>强化学习</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>数据集合</td>\n<td>原始数据<br />[<mark>数千亿</mark>单词：图书、百科、网页等]</td>\n<td>标注用户指令<br />[<mark>数万</mark>用户指令和对应的答案]</td>\n<td>标注对比对<br />[<mark>数万量级</mark>标注对比对]</td>\n<td>用户指令<br />[<mark>十万量级</mark>用户指令]</td>\n</tr>\n<tr>\n<td>算法</td>\n<td>语言模型训练</td>\n<td>语言模型训练</td>\n<td>二分类模型</td>\n<td>强化学习方法</td>\n</tr>\n<tr>\n<td>模型</td>\n<td>基础模型</td>\n<td>SFT模型</td>\n<td>RM模型</td>\n<td>RL模型</td>\n</tr>\n<tr>\n<td>资源需求</td>\n<td>1000+GPU[月]</td>\n<td>1-100GPU[天]</td>\n<td>1-100GPU[天]</td>\n<td>1-100GPU[天]</td>\n</tr>\n</tbody>\n</table>\n<p><strong>有监督微调：</strong></p>\n<p><code v-pre>指令微调(Instruction Tuning)利用少量高质量数据集合，包含用户输入的提示词(Prompt)和对应的理想输出结果。用户输入包括问题、闲聊对话、任务指令等多种形式和任务</code></p>\n<ul>\n<li>如何微调？利用高质量有监督数据，使用与训练阶段相同的语言模型训练算法，在基础语言模型基础上再训练，得到有监督微调模型(SFT模型)</li>\n<li>微调后的效果：具备初步指令理解能力和上下文理解能力，能够完成开放领域问题、阅读理解、翻译、生成代码等能力，也具备一定对未知任务的泛化能力</li>\n</ul>\n<p><strong>下游任务微调：</strong></p>\n<p><code v-pre>DownstreamTaskFine-tuning</code></p>\n<p>目的：在通用语义表示基础上，根据下游任务的特性进行适配</p>\n<p>注意：容易使得模型遗忘预训练阶段学习到的通用语义知识表示，损失模型的通用性和泛化能力，造成灾难性遗忘(CatastrophicForgetting)问题，因此通常采用混合预训练任务损失和下游微调损失的方法来缓解</p>\n<p><strong>奖励建模：</strong></p>\n<p><code v-pre>Reward Modeling</code></p>\n<p>目的：构建一个文本质量对比模型，对于同一个提示词，SFT模型给出的多个不同输出结果的质量进行排序</p>\n<p>注意：RM模型的准确率对于强化学习阶段的效果有至关重要的影响，因此需要大规模训练数据</p>\n<p><strong>强化学习：</strong></p>\n<p><code v-pre>Reinforcement Learning</code></p>\n<p>流程：根据数十万用户给出的提示词，利用在前一阶段训练的RM模型，给出CFT模型对用户提示词补全结果的质量评估，并与语言模型建模目标综合得到更好效果</p>\n<p>该阶段使得基础模型的熵降低，会减少模型输出的多样性</p>\n<ol>\n<li>从数据集中sample一个prompt</li>\n<li>语言模型(policy)生成输出</li>\n<li>使用奖励模型(Environment)计算得分<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>r</mi><mi>θ</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">r\\theta(x,y)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span></span>(Reward)由<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>r</mi><mi>θ</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">r\\theta(x,y)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span></span>使用PPO-ptx算法优化语言模型</li>\n</ol>\n<img src=\"@source/blog/深度学习模型.assets/image-20250418215738655.png\" alt=\"image-20250418215738655\" style=\"zoom: 80%;\" />\n<h4 id=\"llm参数\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#llm参数\"><span>LLM参数</span></a></h4>\n<h5 id=\"采样系数top-k\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#采样系数top-k\"><span>采样系数Top-k</span></a></h5>\n<p><code v-pre>如何预测下一个词</code></p>\n<blockquote>\n<p>在某一解码时间步，固定选取前k个概率对应的词作为候选，并按照概率进行采样</p>\n<p>采样并不代表每次都会选概率最大的，只是概率越大被选中的几率越大</p>\n</blockquote>\n<img src=\"@source/blog/深度学习模型.assets/image-20250419143534028.png\" alt=\"image-20250419143534028\" style=\"zoom: 67%;\" />\n<p><strong>top-k值对解码效果影响：</strong></p>\n<ul>\n<li>k值变大：选择范围变大，输出更加多样化但精确度也会降低</li>\n<li>k值变小：输出更加确定但缺乏多样性</li>\n</ul>\n<p><strong>缺点：</strong></p>\n<ul>\n<li>不会更具词的概率分布动态调整k值</li>\n</ul>\n<h5 id=\"采样系数top-p\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#采样系数top-p\"><span>采样系数Top-p</span></a></h5>\n<blockquote>\n<p>解决了Top-k采样中只能固定选取前k个词的问题</p>\n<p>在某一解码时间步，动态选取概率之和大于p的最小集合作为候选，并按照概率进行采样</p>\n</blockquote>\n<img src=\"@source/blog/深度学习模型.assets/image-20250419143557550.png\" alt=\"image-20250419143557550\" style=\"zoom:67%;\" />\n<p><strong>给定p值时，候选词列表的大小主要由概率分布决定：</strong></p>\n<ul>\n<li>如果模型对下一个词比较确定，则候选词列表会比较小</li>\n<li>反之，概率分布会相对均匀(对下一个词不确定)，此时候选列表会相对大一些</li>\n</ul>\n<p><strong>实际应用：</strong></p>\n<ul>\n<li>将<code v-pre>Top-k</code>和<code v-pre>Top-p</code>方法进行结合，先应用<code v-pre>Top-k</code>，然后应用<code v-pre>Top-p</code></li>\n</ul>\n<h5 id=\"温度系数temperature\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#温度系数temperature\"><span>温度系数Temperature</span></a></h5>\n<p><code v-pre>控制了softmax输出分布，Temperature=1时退化为标准softmax函数</code></p>\n<figure><img src=\"@source/blog/深度学习模型.assets/image-20250419144841787.png\" alt=\"image-20250419144841787\" tabindex=\"0\" loading=\"lazy\"><figcaption>image-20250419144841787</figcaption></figure>\n<p><strong>Temperature对输出结果的影响：</strong></p>\n<ul>\n<li>当Temperature较低时(如0.1/0.2)：模型倾向于选择概率较高的单词，生成的文本较为连贯和准确，但可能显得过于保守，缺乏创造性和多样性</li>\n<li>当Temperature较高时(如0.8/1.0)：模型倾向于选择概率较低的单词，生成的文本较为多样和创造，但可能牺牲了一定的连贯性和准确性</li>\n</ul>\n<p><strong>应用技巧：</strong></p>\n<ul>\n<li>LLM中普遍取值一般为0.2~1.0</li>\n<li>对于多样性要求较高的任务(例如对话、文本生成)可适当提高温度系数</li>\n</ul>\n<h4 id=\"预训练模型分类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#预训练模型分类\"><span>预训练模型分类</span></a></h4>\n<h5 id=\"nlu类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#nlu类\"><span>NLU类</span></a></h5>\n<p><code v-pre>自然语言理解</code></p>\n<ul>\n<li>以<code v-pre>BERT</code>为代表的自编码预训练模型，NLU任务：分配、抽取等</li>\n<li>如何训练？借助特定的预训练任务进行学习，如：掩码语言模型(MLM)、下一个句子预测(NSP)等</li>\n<li>双向语言模型，同时建模上文和下文信息</li>\n<li>代表模型：RoBERTa、ALBERT、ELECTRA、DeBERTa等</li>\n<li>NLU任务特点：输出范围确定、评价方法相对明确</li>\n</ul>\n<h5 id=\"nlg类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#nlg类\"><span>NLG类</span></a></h5>\n<p><code v-pre>自然语言生成</code></p>\n<ul>\n<li>以<code v-pre>GPT</code>为代表的自回归预训练模型，NLG任务：文本生成、生成式摘要、对话等</li>\n<li>如何训练？使用Causal LM训练(N-gram语言模型的自然延申)，无需设计复杂的预训练任务</li>\n<li>单向语言模型，部分模型采用双向编码器和单向编码器结构</li>\n<li>代表模型：GPT系列(Decoder-only)、T5和BART(Encoder-Decoder)等</li>\n<li>NLG任务特点：输出自由度搞、评价方法较难、更具有创造性</li>\n</ul>\n<h4 id=\"模型的涌现能力\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型的涌现能力\"><span>模型的涌现能力</span></a></h4>\n<p><code v-pre>《Emergent Abilities of Large Language Models》</code></p>\n<h5 id=\"基于模型放大\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#基于模型放大\"><span>基于模型放大</span></a></h5>\n<ul>\n<li>TruthfulQA：当模型放大至280B，其效果会突然高于随机20%</li>\n<li>Multi-task language understanding：当训练计算量达到70B-280B后效果将远远超过随机</li>\n<li>Word in Context：当PaLM被缩放至540B时，高于随机的效果出现</li>\n</ul>\n<p>根据文章：<code v-pre>Scaling Laws</code> for Neural Language Models</p>\n<img src=\"@source/blog/深度学习模型.assets/image-20250418213352077.png\" alt=\"image-20250418213352077\" style=\"zoom:67%;\" />\n<h5 id=\"基于样例提示\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#基于样例提示\"><span>基于样例提示</span></a></h5>\n<p>通过 few-shot prompting来执行任务的能力也是一种涌现现象</p>\n<h3 id=\"transformer\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#transformer\"><span>transformer</span></a></h3>\n<figure><img src=\"@source/blog/深度学习模型.assets/3319e3d6922a2e7f2499a3130d3b5925.png\" alt=\"在这里插入图片描述\" tabindex=\"0\" loading=\"lazy\"><figcaption>在这里插入图片描述</figcaption></figure>\n<ul>\n<li>BERT（Bidirectional Encoder Representations from Transformers）：双向语言理解模型，仅使用 编码器，用于理解整个句子的上下文，适合分类、问答等理解类任务。</li>\n<li>GPT（Generative Pre-trained Transformer）：自回归语言模型，仅使用 解码器，其设计目的是生成下一个词，适合用于生成式任务，如文本生成、对话等。</li>\n</ul>\n<h3 id=\"多模态\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#多模态\"><span>多模态</span></a></h3>\n<h4 id=\"基本结构\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#基本结构\"><span>基本结构</span></a></h4>\n<h5 id=\"vit\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#vit\"><span>vit</span></a></h5>\n<img src=\"@source/blog/深度学习模型.assets/image-20250504143710226.png\" alt=\"image-20250504143710226\" style=\"zoom:67%;\" />\n<h5 id=\"yolos\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolos\"><span>yolos</span></a></h5>\n<img src=\"@source/blog/深度学习模型.assets/image-20250504143729206.png\" alt=\"image-20250504143729206\" style=\"zoom: 80%;\" />\n<h4 id=\"图文匹配\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#图文匹配\"><span>图文匹配</span></a></h4>\n<h5 id=\"clip\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#clip\"><span>clip</span></a></h5>\n<img src=\"@source/blog/深度学习模型.assets/image-20250504143559792.png\" alt=\"image-20250504143559792\" style=\"zoom:67%;\" />\n<img src=\"@source/blog/深度学习模型.assets/image-20250504150014695.png\" alt=\"image-20250504150014695\" style=\"zoom:80%;\" />\n<figure><img src=\"@source/blog/深度学习模型.assets/image-20250504150103155.png\" alt=\"image-20250504150103155\" tabindex=\"0\" loading=\"lazy\"><figcaption>image-20250504150103155</figcaption></figure>\n<h5 id=\"bridge-tower\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#bridge-tower\"><span>bridge tower</span></a></h5>\n<img src=\"@source/blog/深度学习模型.assets/image-20250504160101916.png\" alt=\"image-20250504160101916\" style=\"zoom:80%;\" />\n<img src=\"@source/blog/深度学习模型.assets/image-20250504160137868.png\" alt=\"image-20250504160137868\" style=\"zoom: 67%;\" />\n<h5 id=\"gpt4原理\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#gpt4原理\"><span>gpt4原理</span></a></h5>\n<img src=\"@source/blog/深度学习模型.assets/image-20250504160229505.png\" alt=\"image-20250504160229505\" style=\"zoom:67%;\" />\n<h4 id=\"文生图-图生文\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#文生图-图生文\"><span>文生图/图生文</span></a></h4>\n<h5 id=\"dall-·-e\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#dall-·-e\"><span>DALL · E</span></a></h5>\n<img src=\"@source/blog/深度学习模型.assets/image-20250504154828197.png\" alt=\"image-20250504154828197\" style=\"zoom:80%;\" />\n<img src=\"@source/blog/深度学习模型.assets/image-20250504154919514.png\" alt=\"image-20250504154919514\" style=\"zoom:80%;\" />\n<h4 id=\"扩散模型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#扩散模型\"><span>扩散模型</span></a></h4>\n<h2 id=\"模型微调\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型微调\"><span>模型微调</span></a></h2>\n<p><code v-pre>处理不当，很可能造成模型原始能力的灾难性以往、即回导致模型原始能力丢失，对于复杂模型更是如此</code></p>\n<h3 id=\"微调概念\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#微调概念\"><span>微调概念</span></a></h3>\n<h4 id=\"全量微调\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#全量微调\"><span>全量微调</span></a></h4>\n<ul>\n<li>对所有参数进行微调</li>\n<li>对算力和显存要求高</li>\n<li>效果最佳</li>\n</ul>\n<h4 id=\"局部微调\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#局部微调\"><span>局部微调</span></a></h4>\n<blockquote>\n<p>重要调整输入输出层效果明显，而非中间层</p>\n</blockquote>\n<ul>\n<li>只调整某些<strong>某部分参数</strong>，例如输入层，输出层或某些特殊层</li>\n<li>对算力和显存要求一般</li>\n<li>一定是有效的</li>\n</ul>\n<h4 id=\"增量微调\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#增量微调\"><span>增量微调</span></a></h4>\n<blockquote>\n<p>前/后增量：</p>\n<ol>\n<li>改变任务（如：2-&gt;8分类），选择后增</li>\n<li>模型是一个提取特征的过程，后增效果好</li>\n</ol>\n</blockquote>\n<ul>\n<li>通过<strong>新增参数</strong>的方式进行微调，新的知识存储在新的参数中。</li>\n<li>对显存和算力要求低</li>\n<li>效果不如全量微调</li>\n</ul>\n<h3 id=\"微调方式\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#微调方式\"><span>微调方式</span></a></h3>\n<h4 id=\"lora-low-rank-adaption\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#lora-low-rank-adaption\"><span>LoRA（Low-Rank Adaption）</span></a></h4>\n<p>通过引入低秩矩阵来减少微调时的参数量。在预训练的模型中，LoRA通过添加两个小矩阵A和B来近似原始的大矩阵<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">Δ</mi><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Δ</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span></span></span>，从而减少需要更新的参数数量。</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>W</mi><mn>0</mn></msub><mo>+</mo><mi mathvariant=\"normal\">Δ</mi><mi>W</mi><mo>=</mo><msub><mi>W</mi><mn>0</mn></msub><mo>+</mo><mi>B</mi><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">W_0+\\Delta W=W_0+BA\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Δ</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B</span><span class=\"mord mathnormal\">A</span></span></span></span></span></p>\n<ul>\n<li>A和B的秩远小于原始矩阵的秩，从而大大减少了需要更新的参数数量</li>\n</ul>\n<figure><img src=\"@source/blog/深度学习模型.assets/image-20250616215925193.png\" alt=\"image-20250616215925193\" tabindex=\"0\" loading=\"lazy\"><figcaption>image-20250616215925193</figcaption></figure>\n<ul>\n<li>训练时：输入分别与原始权重和两个低秩矩阵进行计算，共同的到最终结果，优化则仅优化A和B</li>\n<li>训练完成后，可以将两个低秩矩阵与原始模型中的权重进行合并，合并后的模型与原始模型无异</li>\n</ul>\n<h4 id=\"qlora\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#qlora\"><span>QLoRA</span></a></h4>\n<p><code v-pre>Quantized Low-Rank Adaption，在LoRA的基础上加入量化技术，减少权重表示的位数，从而降低显存和计算需求</code></p>\n<ul>\n<li>量化：将模型权重量化为低精度（如INT4），减少内存占用，并提高推理和训练速度</li>\n</ul>\n<h3 id=\"微调工具\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#微调工具\"><span>微调工具</span></a></h3>\n<h4 id=\"llama-factory\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#llama-factory\"><span>Llama-Factory</span></a></h4>\n<h4 id=\"xtuner\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#xtuner\"><span>Xtuner</span></a></h4>\n<h3 id=\"性能评估-evalscope\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#性能评估-evalscope\"><span>性能评估：EvalScope</span></a></h3>\n<p>https://evalscope.readthedocs.io/zh-cn/latest/best_practice/qwen3.html#id7</p>\n<h2 id=\"模型部署\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型部署\"><span>模型部署</span></a></h2>\n<h3 id=\"ollama\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#ollama\"><span>Ollama</span></a></h3>\n<blockquote>\n<ul>\n<li>针对个人用户</li>\n<li>必须是gguf格式的模型【GGUF格式一般是量化后的】</li>\n</ul>\n</blockquote>\n<h4 id=\"安装\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#安装\"><span>安装</span></a></h4>\n<p><strong>一键安装</strong></p>\n<div class=\"language-bash line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"bash\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code class=\"language-bash\"><span class=\"line\"><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">curl</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> -fsSL</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> https://ollama.com/install.sh</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> |</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\"> sh</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">sudo</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> vim</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> /etc/systemd/system/ollama.service</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">sudo</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> ufw</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> allow</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> 11434/tcp</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">sudo</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> systemctl</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> daemon-reload</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> &#x26;</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\"> sudo</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> systemctl</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> restart</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> ollama</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><p><strong>ollama.service</strong></p>\n<p><code v-pre>使用显卡进行推理</code></p>\n<div class=\"language-toml line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"toml\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code class=\"language-toml\"><span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">Unit</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">Description</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">O</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">llama Service</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">After</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">n</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">etwork-online.target</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">Service</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">ExecStart</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">/</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">usr/local/bin/ollama serve</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">User</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">o</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">llama</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">Group</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">o</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">llama</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">Restart</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">a</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">lways</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">RestartSec</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">3</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">Environment</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">PATH=/mnt/big_disk_0/spa/.cargo/bin:/mnt/big_disk_0/spa/miniconda3/bin:/mnt/big_disk_0/spa/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/mnt/big_disk_0/spa/.local/bin:/mnt/big_disk_0/spa/bin</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">Environment</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">OLLAMA_HOST=0.0.0.0:11434</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">Environment</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">CUDA_VISIBLE_DEVICES=1,0</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">Install</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">WantedBy</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">d</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">efault.target</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><h4 id=\"使用\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#使用\"><span>使用</span></a></h4>\n<div class=\"language-bash line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"bash\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code class=\"language-bash\"><span class=\"line\"><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">ollama</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> serve</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div></div></div><div class=\"language-bash line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"bash\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code class=\"language-bash\"><span class=\"line\"><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">ollama</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> run</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> 模型:参数B</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div></div></div><h3 id=\"vllm\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#vllm\"><span>Vllm</span></a></h3>\n<blockquote>\n<ul>\n<li>企业专用</li>\n<li>一次只支持部署一个模型</li>\n</ul>\n</blockquote>\n<div class=\"language-bash line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"bash\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code class=\"language-bash\"><span class=\"line\"><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">vllm</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> serve</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> /mnt/big_disk/big_disk_3/spa/Code/Study/llm/XiaomiMiMo/MiMo-7B-RL</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> \\</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">\t--trust_remote_code</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> \\</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">    --tensor-parallel-size</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 4</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> \\</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">    --served-model-name</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> MiMo-7B-RL</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><h3 id=\"lmdeploy\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#lmdeploy\"><span>LMDeploy</span></a></h3>\n<blockquote>\n<ul>\n<li>显存优化比vllm好点</li>\n<li>只支持列表内模型</li>\n</ul>\n</blockquote>\n<div class=\"language-bash line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"bash\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code class=\"language-bash\"><span class=\"line\"><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">lmdeploy</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> serve</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> api_server</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> internlm/internlm2_5-7b-chat</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div></div></div></template>","contentStripped":"<h2 id=\"视觉模型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#视觉模型\"><span>视觉模型</span></a></h2>\n<h3 id=\"分类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#分类\"><span>分类</span></a></h3>\n<h4 id=\"任务\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#任务\"><span>任务</span></a></h4>\n<ul>\n<li>\n<p>图像分类：识别出图中出现的物体类别是什么，其功能主要是用于判断是什么？</p>\n<ul>\n<li>VGG</li>\n<li>GoogleNet</li>\n<li>ResNet</li>\n</ul>\n</li>\n<li>\n<p>图像定位：不仅仅需要识别出是什么物体（即分类）同时需要预测物体的位置信息，也就是单个目标在哪里？是什么？</p>\n<ul>\n<li>RCNN</li>\n<li>Fast RCNN</li>\n<li>Faster RCNN</li>\n</ul>\n</li>\n<li>\n<p>目标检测：多目标的定位，即在一个图片中定位多个目标物体，包括分类和定位，也就是多个目标分别在哪里？分别属于那个类别？</p>\n<ul>\n<li>RCNN</li>\n<li>Fast RNN</li>\n<li>Faster RCNN</li>\n<li>SSD</li>\n<li>YOLO</li>\n</ul>\n</li>\n</ul>\n<img src=\"@source/blog/深度学习模型.assets/image-20250405172047846.png\" alt=\"image-20250405172047846\" style=\"zoom:80%;\" />\n<h4 id=\"模型架构\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型架构\"><span>模型架构</span></a></h4>\n<ul>\n<li>二阶段：R-CNN、Fast R-CNN、Faster-R-CNN、SPP-Net、R-FCN</li>\n<li>一阶段：YOLO、SSD、FPN</li>\n</ul>\n<p>图像分割与目标检测：Cascade R-CNN</p>\n<h3 id=\"参数介绍\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#参数介绍\"><span>参数介绍</span></a></h3>\n<h4 id=\"iou\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#iou\"><span>IOU</span></a></h4>\n<p><code v-pre>两个边界框(bounding box)的重叠度</code></p>\n<img src=\"@source/blog/深度学习模型.assets/image-20250405175924671.png\" alt=\"image-20250405175924671\" style=\"zoom:50%;\" />\n$$\nIOU=\\frac{A\\cap B}{A\\cup B}=\\frac{S_{_{A,B}}}{S_{_A}+S_{_B}-S_{_{A,B}}}\n$$\n<h4 id=\"map\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#map\"><span>MAP</span></a></h4>\n<p><strong>精度和召回率</strong></p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>正例（实际）</th>\n<th>负例（实际）</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>正例（预测）</strong></td>\n<td>TP</td>\n<td>FP</td>\n</tr>\n<tr>\n<td><strong>负例（预测）</strong></td>\n<td>FN</td>\n<td>TN</td>\n</tr>\n</tbody>\n</table>\n<img src=\"@source/blog/深度学习模型.assets/image-20250405180027125.png\" alt=\"image-20250405180027125\" style=\"zoom:67%;\" />\n<p>精度/查准率：</p>\n<p><code v-pre>预测为正例的样本中实际为正例的比例</code></p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">precision = \\frac{TP}{TP+FP}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.854em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">p</span><span class=\"mord mathnormal\">rec</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">s</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\">n</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.1297em;vertical-align:-0.7693em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3603em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">TP</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">FP</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">TP</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7693em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p>召回率/查全率：</p>\n<p><code v-pre>实际为正例的样本中被正确预测的比例</code></p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">recall = \\frac{TP}{TP+FN}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">rec</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">ll</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.1297em;vertical-align:-0.7693em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3603em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">TP</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">FN</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">TP</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7693em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p><strong>mAP</strong></p>\n<ol>\n<li>划定不同阈值计算不同的精度/召回率\n<ul>\n<li>计算在不同阈值的情况下，Predicision和Recall的值。\n<ul>\n<li>阈值0.9：无视所有小于0.9的predict，那么此时TP=1,FP=0,precision=1，所有标签数目为3，那么recall=1/3</li>\n<li>阈值0.8：无视所有小于0.8的predict，那么此时TP=1,FP=1,precision=1/2，所有标签数目为3，那么recall=1/3</li>\n<li>阈值0.7：无视所有小于0.7的predict，那么此时TP=2,FP=1,precision=2/3，所有标签数目为3，那么recall=2/3</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>根据精度/召回率，绘制RP曲线，计算<strong>AP</strong>值\n<ul>\n<li>在每个”峰值点”往左画一条直线，和上一个“峰值点”的垂直线像交，这样和坐标轴框出来的面积就是AP值。</li>\n</ul>\n</li>\n</ol>\n<img src=\"@source/blog/深度学习模型.assets/image-20250405190809088.png\" alt=\"image-20250405190809088\" style=\"zoom:67%;\" />\n<ol start=\"3\">\n<li>mAP：对多个类别的检测情况评估</li>\n</ol>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>m</mi><mi>A</mi><mi>P</mi><mo>=</mo><mfrac><mrow><mo>∑</mo><mi>A</mi><mi>P</mi></mrow><mrow><mi>N</mi><mo stretchy=\"false\">(</mo><mi>c</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi><mi>e</mi><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">mAP=\\frac{\\sum AP}{N(classes)}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">m</span><span class=\"mord mathnormal\">A</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.363em;vertical-align:-0.936em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.427em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">c</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\">sses</span><span class=\"mclose\">)</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:0em;\">∑</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">A</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<h4 id=\"overfeat\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#overfeat\"><span>overfeat</span></a></h4>\n<blockquote>\n<p>时间：2013年</p>\n<p>特点：采用了一种基于 <strong>滑动窗口</strong> 和 <strong>全卷积神经网络（FCN）</strong> 的方法来实现目标的分类和定位</p>\n</blockquote>\n<img src=\"@source/blog/深度学习模型.assets/82e1d2d67195c77b6b755473adc2d542.png\" alt=\"img\" style=\"zoom:50%;\" />\n<h5 id=\"流程\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#流程\"><span>流程</span></a></h5>\n<ol>\n<li>通过FCN全卷积网络提取特征\n<ul>\n<li>首先定义若干个大小窗口（K个）</li>\n<li>K中每个窗口都要滑动图片，每个窗口都需要滑动M次</li>\n<li>得到K x M个特征图</li>\n</ul>\n</li>\n<li>对每个位置的特征图进行目标分类和定位</li>\n<li>输出每个窗口的类别得分和边框坐标</li>\n</ol>\n<h4 id=\"nms\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#nms\"><span>NMS</span></a></h4>\n<blockquote>\n<p><strong>去除冗余的候选框</strong>，只保留最具代表性的框，提升检测的准确性</p>\n</blockquote>\n<ol>\n<li>标准 NMS\n<ul>\n<li>经典的 NMS 计算方法，直接移除冗余框</li>\n<li>对于每个框，按得分降序排列，对所有其他框计算 IOU，并移除重叠框</li>\n</ul>\n</li>\n<li>Soft NMS\n<ul>\n<li>Soft NMS 不直接去除重叠的框，而是<strong>平滑</strong>它们的得分。具体来说，随着重叠度增加，框的得分会逐渐衰减（通过一个衰减因子）</li>\n<li>避免移除可能是正确框的高 IOU 框，尤其是在物体边缘的框</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"two-stage\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#two-stage\"><span>Two-Stage</span></a></h3>\n<h4 id=\"rcnn-region-based-cnn\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#rcnn-region-based-cnn\"><span>RCNN（Region-based CNN）</span></a></h4>\n<img src=\"@source/blog/深度学习模型.assets/image-20250405203801765.png\" alt=\"image-20250405203801765\" style=\"zoom:67%;\" />\n<h5 id=\"流程-1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#流程-1\"><span>流程</span></a></h5>\n<ol>\n<li>生成候选区域：基于颜色、纹理等低级特征合并超像素</li>\n<li>统一候选区域：将每个候选区域缩放至固定尺寸</li>\n<li>提取特征：使用预训练的CNN（如AlexNet）提特征</li>\n<li>分类与回归：使用SVM分类/线性回归修正边界框</li>\n</ol>\n<h4 id=\"sppnet\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#sppnet\"><span>SPPNet</span></a></h4>\n<img src=\"@source/blog/深度学习模型.assets/image-20250405204318004.png\" alt=\"image-20250405204318004\" style=\"zoom:67%;\" />\n<h5 id=\"流程-2\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#流程-2\"><span>流程</span></a></h5>\n<ol>\n<li>整图输入CNN生成特征图。</li>\n<li>候选区域通过坐标映射到特征图对应位置。</li>\n<li>使用SPP层（如4级金字塔：1x1、2x2、3x3、6x6）池化</li>\n<li>分类与回归：使用SVM分类/线性回归修正边界框</li>\n</ol>\n<h4 id=\"fast-rcnn\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#fast-rcnn\"><span>Fast RCNN</span></a></h4>\n<img src=\"@source/blog/深度学习模型.assets/image-20250405212622317.png\" alt=\"image-20250405212622317\" style=\"zoom:67%;\" />\n<h5 id=\"流程-3\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#流程-3\"><span>流程</span></a></h5>\n<ol>\n<li>整图输入CNN生成特征图</li>\n<li>用<strong>Selective Search（选择性搜索算法</strong>）生成候选框(ROIs)</li>\n<li>RoI Pooling将不同尺寸候选框(ROIs)映射到特征图并池化为统一特征图大小</li>\n<li>两个全连接层分别输出分类结果和边界框偏移量</li>\n<li>多任务损失训练分类与回归网络</li>\n</ol>\n<h4 id=\"faster-rcnn\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#faster-rcnn\"><span><mark>Faster RCNN</mark></span></a></h4>\n<img src=\"@source/blog/深度学习模型.assets/image-20250406141359665.png\" alt=\"image-20250406141359665\" style=\"zoom:67%;\" />\n<img src=\"@source/blog/深度学习模型.assets/image-20250406141705239.png\" alt=\"image-20250406141705239\" style=\"zoom: 150%;\" />\n<h5 id=\"流程-4\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#流程-4\"><span>流程</span></a></h5>\n<ol>\n<li>整图输入CNN生成特征图。</li>\n<li><strong>RPN</strong>生成候选框并过滤（NMS去除低质量候选框 + 高精度低召回率 = 量少质优~300个）</li>\n<li>RoI Pooling将候选框映射到特征图并池化</li>\n<li>两个全连接层分别输出分类结果和边界框偏移量</li>\n<li>多任务损失训练分类与回归网络</li>\n</ol>\n<h4 id=\"对比总结\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#对比总结\"><span>对比总结</span></a></h4>\n<img src=\"@source/blog/深度学习模型.assets/image-20250406144031898.png\" alt=\"image-20250406144031898\" style=\"zoom:67%;\" />\n<img src=\"@source/blog/深度学习模型.assets/image-20250406144052203.png\" alt=\"image-20250406144052203\" style=\"zoom:67%;\" />\n<img src=\"@source/blog/深度学习模型.assets/image-20250406144124733.png\" alt=\"image-20250406144124733\" style=\"zoom:67%;\" />\n<p><strong>R-CNN网络演进：</strong></p>\n<img src=\"@source/blog/深度学习模型.assets/image-20250406144412151.png\" alt=\"image-20250406144412151\" style=\"zoom:80%;\" />\n<h4 id=\"rfcn\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#rfcn\"><span>RFCN</span></a></h4>\n<h3 id=\"one-stage\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#one-stage\"><span>One-Stage</span></a></h3>\n<h4 id=\"ssd\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#ssd\"><span><mark>SSD</mark></span></a></h4>\n<img src=\"@source/blog/深度学习模型.assets/image-20250406162020930.png\" alt=\"image-20250406162020930\" style=\"zoom:67%;\" />\n<h5 id=\"流程-5\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#流程-5\"><span>流程</span></a></h5>\n<ol>\n<li>整图输入基础网络（如VGG）生成多尺度特征图。</li>\n<li>每个特征图位置预测K个Anchor的类别和坐标偏移。</li>\n<li>通过NMS筛选最终检测框。</li>\n</ol>\n<h4 id=\"yolov1\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolov1\"><span>YOLOv1</span></a></h4>\n<p><code v-pre>输入图片：448*448*3</code></p>\n<figure><img src=\"@source/blog/深度学习模型.assets/a29c47bca5ec4f359b54fc6d313879be.png\" alt=\"a29c47bca5ec4f359b54fc6d313879be\" tabindex=\"0\" loading=\"lazy\"><figcaption>a29c47bca5ec4f359b54fc6d313879be</figcaption></figure>\n<p>每个小框预测位置信息（<code v-pre>x, y, w, h, c</code>）+  类别概率信息<code v-pre>C</code></p>\n<ul>\n<li>x：coordinate of bbox center inside cell([0;1] wrt grid cell size)</li>\n<li>y：coordinate of bbox center inside cell([0;1] wrt grid cell size)</li>\n<li>w：bbox width ([0;1] wrt image)</li>\n<li>h：bbox width ([0;1] wrt image)</li>\n<li>c：bbox confidence ~ P(obj in bbox1)</li>\n<li>C：C个不同类别概率信息</li>\n</ul>\n<h4 id=\"yolov2\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolov2\"><span>YOLOv2</span></a></h4>\n<h4 id=\"yolov3\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolov3\"><span>YOLOv3</span></a></h4>\n<h4 id=\"yolov4\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolov4\"><span>YOLOv4</span></a></h4>\n<h4 id=\"yolov5\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolov5\"><span>YOLOv5</span></a></h4>\n<figure><img src=\"@source/blog/深度学习模型.assets/image-20250413193305078.png\" alt=\"image-20250413193305078\" tabindex=\"0\" loading=\"lazy\"><figcaption>image-20250413193305078</figcaption></figure>\n<figure><img src=\"@source/blog/深度学习模型.assets/image-20250413190728754.png\" alt=\"image-20250413190728754\" tabindex=\"0\" loading=\"lazy\"><figcaption>image-20250413190728754</figcaption></figure>\n<figure><img src=\"@source/blog/深度学习模型.assets/image-20250413190742275.png\" alt=\"image-20250413190742275\" tabindex=\"0\" loading=\"lazy\"><figcaption>image-20250413190742275</figcaption></figure>\n<h4 id=\"yolov6\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolov6\"><span>YOLOv6</span></a></h4>\n<h4 id=\"yolov7\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolov7\"><span>YOLOv7</span></a></h4>\n<h4 id=\"yolov8\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolov8\"><span>YOLOv8</span></a></h4>\n<h4 id=\"yolov12\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolov12\"><span>YOLOv12</span></a></h4>\n<h2 id=\"大模型与多模态\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#大模型与多模态\"><span>大模型与多模态</span></a></h2>\n<h3 id=\"概述\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#概述\"><span>概述</span></a></h3>\n<p><code v-pre>基本Pipeline：问题明确-&gt;数据获取-&gt;数据清理-&gt;数据探索-&gt;数据准备-&gt;训练模型-&gt;微调模型-&gt;结果应用-&gt;监控迭代</code></p>\n<h4 id=\"整体视角\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#整体视角\"><span>整体视角</span></a></h4>\n<ul>\n<li>数据决定算法的上线，模型只是去逼近这个上线</li>\n<li>算法工程师的基础能力：数据采集、评估、传输、预处理、标注、分析、挖掘、特征融合等</li>\n</ul>\n<h4 id=\"llm构建流程\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#llm构建流程\"><span>LLM构建流程</span></a></h4>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>预训练</th>\n<th>有监督微调</th>\n<th>奖励建模</th>\n<th>强化学习</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>数据集合</td>\n<td>原始数据<br />[<mark>数千亿</mark>单词：图书、百科、网页等]</td>\n<td>标注用户指令<br />[<mark>数万</mark>用户指令和对应的答案]</td>\n<td>标注对比对<br />[<mark>数万量级</mark>标注对比对]</td>\n<td>用户指令<br />[<mark>十万量级</mark>用户指令]</td>\n</tr>\n<tr>\n<td>算法</td>\n<td>语言模型训练</td>\n<td>语言模型训练</td>\n<td>二分类模型</td>\n<td>强化学习方法</td>\n</tr>\n<tr>\n<td>模型</td>\n<td>基础模型</td>\n<td>SFT模型</td>\n<td>RM模型</td>\n<td>RL模型</td>\n</tr>\n<tr>\n<td>资源需求</td>\n<td>1000+GPU[月]</td>\n<td>1-100GPU[天]</td>\n<td>1-100GPU[天]</td>\n<td>1-100GPU[天]</td>\n</tr>\n</tbody>\n</table>\n<p><strong>有监督微调：</strong></p>\n<p><code v-pre>指令微调(Instruction Tuning)利用少量高质量数据集合，包含用户输入的提示词(Prompt)和对应的理想输出结果。用户输入包括问题、闲聊对话、任务指令等多种形式和任务</code></p>\n<ul>\n<li>如何微调？利用高质量有监督数据，使用与训练阶段相同的语言模型训练算法，在基础语言模型基础上再训练，得到有监督微调模型(SFT模型)</li>\n<li>微调后的效果：具备初步指令理解能力和上下文理解能力，能够完成开放领域问题、阅读理解、翻译、生成代码等能力，也具备一定对未知任务的泛化能力</li>\n</ul>\n<p><strong>下游任务微调：</strong></p>\n<p><code v-pre>DownstreamTaskFine-tuning</code></p>\n<p>目的：在通用语义表示基础上，根据下游任务的特性进行适配</p>\n<p>注意：容易使得模型遗忘预训练阶段学习到的通用语义知识表示，损失模型的通用性和泛化能力，造成灾难性遗忘(CatastrophicForgetting)问题，因此通常采用混合预训练任务损失和下游微调损失的方法来缓解</p>\n<p><strong>奖励建模：</strong></p>\n<p><code v-pre>Reward Modeling</code></p>\n<p>目的：构建一个文本质量对比模型，对于同一个提示词，SFT模型给出的多个不同输出结果的质量进行排序</p>\n<p>注意：RM模型的准确率对于强化学习阶段的效果有至关重要的影响，因此需要大规模训练数据</p>\n<p><strong>强化学习：</strong></p>\n<p><code v-pre>Reinforcement Learning</code></p>\n<p>流程：根据数十万用户给出的提示词，利用在前一阶段训练的RM模型，给出CFT模型对用户提示词补全结果的质量评估，并与语言模型建模目标综合得到更好效果</p>\n<p>该阶段使得基础模型的熵降低，会减少模型输出的多样性</p>\n<ol>\n<li>从数据集中sample一个prompt</li>\n<li>语言模型(policy)生成输出</li>\n<li>使用奖励模型(Environment)计算得分<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>r</mi><mi>θ</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">r\\theta(x,y)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span></span>(Reward)由<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>r</mi><mi>θ</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">r\\theta(x,y)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span></span>使用PPO-ptx算法优化语言模型</li>\n</ol>\n<img src=\"@source/blog/深度学习模型.assets/image-20250418215738655.png\" alt=\"image-20250418215738655\" style=\"zoom: 80%;\" />\n<h4 id=\"llm参数\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#llm参数\"><span>LLM参数</span></a></h4>\n<h5 id=\"采样系数top-k\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#采样系数top-k\"><span>采样系数Top-k</span></a></h5>\n<p><code v-pre>如何预测下一个词</code></p>\n<blockquote>\n<p>在某一解码时间步，固定选取前k个概率对应的词作为候选，并按照概率进行采样</p>\n<p>采样并不代表每次都会选概率最大的，只是概率越大被选中的几率越大</p>\n</blockquote>\n<img src=\"@source/blog/深度学习模型.assets/image-20250419143534028.png\" alt=\"image-20250419143534028\" style=\"zoom: 67%;\" />\n<p><strong>top-k值对解码效果影响：</strong></p>\n<ul>\n<li>k值变大：选择范围变大，输出更加多样化但精确度也会降低</li>\n<li>k值变小：输出更加确定但缺乏多样性</li>\n</ul>\n<p><strong>缺点：</strong></p>\n<ul>\n<li>不会更具词的概率分布动态调整k值</li>\n</ul>\n<h5 id=\"采样系数top-p\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#采样系数top-p\"><span>采样系数Top-p</span></a></h5>\n<blockquote>\n<p>解决了Top-k采样中只能固定选取前k个词的问题</p>\n<p>在某一解码时间步，动态选取概率之和大于p的最小集合作为候选，并按照概率进行采样</p>\n</blockquote>\n<img src=\"@source/blog/深度学习模型.assets/image-20250419143557550.png\" alt=\"image-20250419143557550\" style=\"zoom:67%;\" />\n<p><strong>给定p值时，候选词列表的大小主要由概率分布决定：</strong></p>\n<ul>\n<li>如果模型对下一个词比较确定，则候选词列表会比较小</li>\n<li>反之，概率分布会相对均匀(对下一个词不确定)，此时候选列表会相对大一些</li>\n</ul>\n<p><strong>实际应用：</strong></p>\n<ul>\n<li>将<code v-pre>Top-k</code>和<code v-pre>Top-p</code>方法进行结合，先应用<code v-pre>Top-k</code>，然后应用<code v-pre>Top-p</code></li>\n</ul>\n<h5 id=\"温度系数temperature\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#温度系数temperature\"><span>温度系数Temperature</span></a></h5>\n<p><code v-pre>控制了softmax输出分布，Temperature=1时退化为标准softmax函数</code></p>\n<figure><img src=\"@source/blog/深度学习模型.assets/image-20250419144841787.png\" alt=\"image-20250419144841787\" tabindex=\"0\" loading=\"lazy\"><figcaption>image-20250419144841787</figcaption></figure>\n<p><strong>Temperature对输出结果的影响：</strong></p>\n<ul>\n<li>当Temperature较低时(如0.1/0.2)：模型倾向于选择概率较高的单词，生成的文本较为连贯和准确，但可能显得过于保守，缺乏创造性和多样性</li>\n<li>当Temperature较高时(如0.8/1.0)：模型倾向于选择概率较低的单词，生成的文本较为多样和创造，但可能牺牲了一定的连贯性和准确性</li>\n</ul>\n<p><strong>应用技巧：</strong></p>\n<ul>\n<li>LLM中普遍取值一般为0.2~1.0</li>\n<li>对于多样性要求较高的任务(例如对话、文本生成)可适当提高温度系数</li>\n</ul>\n<h4 id=\"预训练模型分类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#预训练模型分类\"><span>预训练模型分类</span></a></h4>\n<h5 id=\"nlu类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#nlu类\"><span>NLU类</span></a></h5>\n<p><code v-pre>自然语言理解</code></p>\n<ul>\n<li>以<code v-pre>BERT</code>为代表的自编码预训练模型，NLU任务：分配、抽取等</li>\n<li>如何训练？借助特定的预训练任务进行学习，如：掩码语言模型(MLM)、下一个句子预测(NSP)等</li>\n<li>双向语言模型，同时建模上文和下文信息</li>\n<li>代表模型：RoBERTa、ALBERT、ELECTRA、DeBERTa等</li>\n<li>NLU任务特点：输出范围确定、评价方法相对明确</li>\n</ul>\n<h5 id=\"nlg类\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#nlg类\"><span>NLG类</span></a></h5>\n<p><code v-pre>自然语言生成</code></p>\n<ul>\n<li>以<code v-pre>GPT</code>为代表的自回归预训练模型，NLG任务：文本生成、生成式摘要、对话等</li>\n<li>如何训练？使用Causal LM训练(N-gram语言模型的自然延申)，无需设计复杂的预训练任务</li>\n<li>单向语言模型，部分模型采用双向编码器和单向编码器结构</li>\n<li>代表模型：GPT系列(Decoder-only)、T5和BART(Encoder-Decoder)等</li>\n<li>NLG任务特点：输出自由度搞、评价方法较难、更具有创造性</li>\n</ul>\n<h4 id=\"模型的涌现能力\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型的涌现能力\"><span>模型的涌现能力</span></a></h4>\n<p><code v-pre>《Emergent Abilities of Large Language Models》</code></p>\n<h5 id=\"基于模型放大\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#基于模型放大\"><span>基于模型放大</span></a></h5>\n<ul>\n<li>TruthfulQA：当模型放大至280B，其效果会突然高于随机20%</li>\n<li>Multi-task language understanding：当训练计算量达到70B-280B后效果将远远超过随机</li>\n<li>Word in Context：当PaLM被缩放至540B时，高于随机的效果出现</li>\n</ul>\n<p>根据文章：<code v-pre>Scaling Laws</code> for Neural Language Models</p>\n<img src=\"@source/blog/深度学习模型.assets/image-20250418213352077.png\" alt=\"image-20250418213352077\" style=\"zoom:67%;\" />\n<h5 id=\"基于样例提示\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#基于样例提示\"><span>基于样例提示</span></a></h5>\n<p>通过 few-shot prompting来执行任务的能力也是一种涌现现象</p>\n<h3 id=\"transformer\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#transformer\"><span>transformer</span></a></h3>\n<figure><img src=\"@source/blog/深度学习模型.assets/3319e3d6922a2e7f2499a3130d3b5925.png\" alt=\"在这里插入图片描述\" tabindex=\"0\" loading=\"lazy\"><figcaption>在这里插入图片描述</figcaption></figure>\n<ul>\n<li>BERT（Bidirectional Encoder Representations from Transformers）：双向语言理解模型，仅使用 编码器，用于理解整个句子的上下文，适合分类、问答等理解类任务。</li>\n<li>GPT（Generative Pre-trained Transformer）：自回归语言模型，仅使用 解码器，其设计目的是生成下一个词，适合用于生成式任务，如文本生成、对话等。</li>\n</ul>\n<h3 id=\"多模态\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#多模态\"><span>多模态</span></a></h3>\n<h4 id=\"基本结构\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#基本结构\"><span>基本结构</span></a></h4>\n<h5 id=\"vit\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#vit\"><span>vit</span></a></h5>\n<img src=\"@source/blog/深度学习模型.assets/image-20250504143710226.png\" alt=\"image-20250504143710226\" style=\"zoom:67%;\" />\n<h5 id=\"yolos\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#yolos\"><span>yolos</span></a></h5>\n<img src=\"@source/blog/深度学习模型.assets/image-20250504143729206.png\" alt=\"image-20250504143729206\" style=\"zoom: 80%;\" />\n<h4 id=\"图文匹配\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#图文匹配\"><span>图文匹配</span></a></h4>\n<h5 id=\"clip\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#clip\"><span>clip</span></a></h5>\n<img src=\"@source/blog/深度学习模型.assets/image-20250504143559792.png\" alt=\"image-20250504143559792\" style=\"zoom:67%;\" />\n<img src=\"@source/blog/深度学习模型.assets/image-20250504150014695.png\" alt=\"image-20250504150014695\" style=\"zoom:80%;\" />\n<figure><img src=\"@source/blog/深度学习模型.assets/image-20250504150103155.png\" alt=\"image-20250504150103155\" tabindex=\"0\" loading=\"lazy\"><figcaption>image-20250504150103155</figcaption></figure>\n<h5 id=\"bridge-tower\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#bridge-tower\"><span>bridge tower</span></a></h5>\n<img src=\"@source/blog/深度学习模型.assets/image-20250504160101916.png\" alt=\"image-20250504160101916\" style=\"zoom:80%;\" />\n<img src=\"@source/blog/深度学习模型.assets/image-20250504160137868.png\" alt=\"image-20250504160137868\" style=\"zoom: 67%;\" />\n<h5 id=\"gpt4原理\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#gpt4原理\"><span>gpt4原理</span></a></h5>\n<img src=\"@source/blog/深度学习模型.assets/image-20250504160229505.png\" alt=\"image-20250504160229505\" style=\"zoom:67%;\" />\n<h4 id=\"文生图-图生文\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#文生图-图生文\"><span>文生图/图生文</span></a></h4>\n<h5 id=\"dall-·-e\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#dall-·-e\"><span>DALL · E</span></a></h5>\n<img src=\"@source/blog/深度学习模型.assets/image-20250504154828197.png\" alt=\"image-20250504154828197\" style=\"zoom:80%;\" />\n<img src=\"@source/blog/深度学习模型.assets/image-20250504154919514.png\" alt=\"image-20250504154919514\" style=\"zoom:80%;\" />\n<h4 id=\"扩散模型\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#扩散模型\"><span>扩散模型</span></a></h4>\n<h2 id=\"模型微调\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型微调\"><span>模型微调</span></a></h2>\n<p><code v-pre>处理不当，很可能造成模型原始能力的灾难性以往、即回导致模型原始能力丢失，对于复杂模型更是如此</code></p>\n<h3 id=\"微调概念\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#微调概念\"><span>微调概念</span></a></h3>\n<h4 id=\"全量微调\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#全量微调\"><span>全量微调</span></a></h4>\n<ul>\n<li>对所有参数进行微调</li>\n<li>对算力和显存要求高</li>\n<li>效果最佳</li>\n</ul>\n<h4 id=\"局部微调\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#局部微调\"><span>局部微调</span></a></h4>\n<blockquote>\n<p>重要调整输入输出层效果明显，而非中间层</p>\n</blockquote>\n<ul>\n<li>只调整某些<strong>某部分参数</strong>，例如输入层，输出层或某些特殊层</li>\n<li>对算力和显存要求一般</li>\n<li>一定是有效的</li>\n</ul>\n<h4 id=\"增量微调\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#增量微调\"><span>增量微调</span></a></h4>\n<blockquote>\n<p>前/后增量：</p>\n<ol>\n<li>改变任务（如：2-&gt;8分类），选择后增</li>\n<li>模型是一个提取特征的过程，后增效果好</li>\n</ol>\n</blockquote>\n<ul>\n<li>通过<strong>新增参数</strong>的方式进行微调，新的知识存储在新的参数中。</li>\n<li>对显存和算力要求低</li>\n<li>效果不如全量微调</li>\n</ul>\n<h3 id=\"微调方式\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#微调方式\"><span>微调方式</span></a></h3>\n<h4 id=\"lora-low-rank-adaption\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#lora-low-rank-adaption\"><span>LoRA（Low-Rank Adaption）</span></a></h4>\n<p>通过引入低秩矩阵来减少微调时的参数量。在预训练的模型中，LoRA通过添加两个小矩阵A和B来近似原始的大矩阵<span v-pre class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">Δ</mi><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Δ</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span></span></span>，从而减少需要更新的参数数量。</p>\n<p v-pre class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>W</mi><mn>0</mn></msub><mo>+</mo><mi mathvariant=\"normal\">Δ</mi><mi>W</mi><mo>=</mo><msub><mi>W</mi><mn>0</mn></msub><mo>+</mo><mi>B</mi><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">W_0+\\Delta W=W_0+BA\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Δ</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B</span><span class=\"mord mathnormal\">A</span></span></span></span></span></p>\n<ul>\n<li>A和B的秩远小于原始矩阵的秩，从而大大减少了需要更新的参数数量</li>\n</ul>\n<figure><img src=\"@source/blog/深度学习模型.assets/image-20250616215925193.png\" alt=\"image-20250616215925193\" tabindex=\"0\" loading=\"lazy\"><figcaption>image-20250616215925193</figcaption></figure>\n<ul>\n<li>训练时：输入分别与原始权重和两个低秩矩阵进行计算，共同的到最终结果，优化则仅优化A和B</li>\n<li>训练完成后，可以将两个低秩矩阵与原始模型中的权重进行合并，合并后的模型与原始模型无异</li>\n</ul>\n<h4 id=\"qlora\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#qlora\"><span>QLoRA</span></a></h4>\n<p><code v-pre>Quantized Low-Rank Adaption，在LoRA的基础上加入量化技术，减少权重表示的位数，从而降低显存和计算需求</code></p>\n<ul>\n<li>量化：将模型权重量化为低精度（如INT4），减少内存占用，并提高推理和训练速度</li>\n</ul>\n<h3 id=\"微调工具\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#微调工具\"><span>微调工具</span></a></h3>\n<h4 id=\"llama-factory\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#llama-factory\"><span>Llama-Factory</span></a></h4>\n<h4 id=\"xtuner\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#xtuner\"><span>Xtuner</span></a></h4>\n<h3 id=\"性能评估-evalscope\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#性能评估-evalscope\"><span>性能评估：EvalScope</span></a></h3>\n<p>https://evalscope.readthedocs.io/zh-cn/latest/best_practice/qwen3.html#id7</p>\n<h2 id=\"模型部署\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#模型部署\"><span>模型部署</span></a></h2>\n<h3 id=\"ollama\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#ollama\"><span>Ollama</span></a></h3>\n<blockquote>\n<ul>\n<li>针对个人用户</li>\n<li>必须是gguf格式的模型【GGUF格式一般是量化后的】</li>\n</ul>\n</blockquote>\n<h4 id=\"安装\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#安装\"><span>安装</span></a></h4>\n<p><strong>一键安装</strong></p>\n<div class=\"language-bash line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"bash\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code class=\"language-bash\"><span class=\"line\"><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">curl</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> -fsSL</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> https://ollama.com/install.sh</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> |</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\"> sh</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">sudo</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> vim</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> /etc/systemd/system/ollama.service</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">sudo</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> ufw</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> allow</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> 11434/tcp</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">sudo</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> systemctl</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> daemon-reload</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> &#x26;</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\"> sudo</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> systemctl</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> restart</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> ollama</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><p><strong>ollama.service</strong></p>\n<p><code v-pre>使用显卡进行推理</code></p>\n<div class=\"language-toml line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"toml\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code class=\"language-toml\"><span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">Unit</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">Description</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">O</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">llama Service</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">After</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">n</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">etwork-online.target</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">Service</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">ExecStart</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">/</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">usr/local/bin/ollama serve</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">User</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">o</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">llama</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">Group</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">o</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">llama</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">Restart</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">a</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">lways</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">RestartSec</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\">3</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">Environment</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">PATH=/mnt/big_disk_0/spa/.cargo/bin:/mnt/big_disk_0/spa/miniconda3/bin:/mnt/big_disk_0/spa/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/mnt/big_disk_0/spa/.local/bin:/mnt/big_disk_0/spa/bin</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">Environment</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">OLLAMA_HOST=0.0.0.0:11434</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">Environment</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">CUDA_VISIBLE_DEVICES=1,0</span><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">Install</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B07D48;--shiki-dark:#BD976A\">WantedBy</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">=</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">d</span><span style=\"--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic\">efault.target</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><h4 id=\"使用\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#使用\"><span>使用</span></a></h4>\n<div class=\"language-bash line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"bash\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code class=\"language-bash\"><span class=\"line\"><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">ollama</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> serve</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div></div></div><div class=\"language-bash line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"bash\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code class=\"language-bash\"><span class=\"line\"><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">ollama</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> run</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> 模型:参数B</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div></div></div><h3 id=\"vllm\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#vllm\"><span>Vllm</span></a></h3>\n<blockquote>\n<ul>\n<li>企业专用</li>\n<li>一次只支持部署一个模型</li>\n</ul>\n</blockquote>\n<div class=\"language-bash line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"bash\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code class=\"language-bash\"><span class=\"line\"><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">vllm</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> serve</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> /mnt/big_disk/big_disk_3/spa/Code/Study/llm/XiaomiMiMo/MiMo-7B-RL</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> \\</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">\t--trust_remote_code</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> \\</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">    --tensor-parallel-size</span><span style=\"--shiki-light:#2F798A;--shiki-dark:#4C9A91\"> 4</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\"> \\</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">    --served-model-name</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> MiMo-7B-RL</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><h3 id=\"lmdeploy\" tabindex=\"-1\"><a class=\"header-anchor\" href=\"#lmdeploy\"><span>LMDeploy</span></a></h3>\n<blockquote>\n<ul>\n<li>显存优化比vllm好点</li>\n<li>只支持列表内模型</li>\n</ul>\n</blockquote>\n<div class=\"language-bash line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"bash\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code class=\"language-bash\"><span class=\"line\"><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">lmdeploy</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> serve</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> api_server</span><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\"> internlm/internlm2_5-7b-chat</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div></div></div>","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"## 视觉模型\n\n### 分类\n\n#### 任务\n\n- 图像分类：识别出图中出现的物体类别是什么，其功能主要是用于判断是什么？\n  - VGG\n  - GoogleNet\n  - ResNet\n\n- 图像定位：不仅仅需要识别出是什么物体（即分类）同时需要预测物体的位置信息，也就是单个目标在哪里？是什么？\n  - RCNN\n  - Fast RCNN\n  - Faster RCNN\n\n- 目标检测：多目标的定位，即在一个图片中定位多个目标物体，包括分类和定位，也就是多个目标分别在哪里？分别属于那个类别？\n  - RCNN\n  - Fast RNN\n  - Faster RCNN\n  - SSD\n  - YOLO\n\n<img src=\"./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250405172047846.png\" alt=\"image-20250405172047846\" style=\"zoom:80%;\" />\n\n#### 模型架构\n\n- 二阶段：R-CNN、Fast R-CNN、Faster-R-CNN、SPP-Net、R-FCN\n- 一阶段：YOLO、SSD、FPN\n\n图像分割与目标检测：Cascade R-CNN\n\n### 参数介绍\n\n#### IOU\n\n`两个边界框(bounding box)的重叠度`\n\n<img src=\"./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250405175924671.png\" alt=\"image-20250405175924671\" style=\"zoom:50%;\" />\n$$\nIOU=\\frac{A\\cap B}{A\\cup B}=\\frac{S_{_{A,B}}}{S_{_A}+S_{_B}-S_{_{A,B}}}\n$$\n\n#### MAP\n\n**精度和召回率**\n\n|                  | 正例（实际） | 负例（实际） |\n| ---------------- | ------------ | ------------ |\n| **正例（预测）** | TP           | FP           |\n| **负例（预测）** | FN           | TN           |\n\n<img src=\"./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250405180027125.png\" alt=\"image-20250405180027125\" style=\"zoom:67%;\" />\n\n精度/查准率：\n\n`预测为正例的样本中实际为正例的比例`\n$$\nprecision = \\frac{TP}{TP+FP}\n$$\n召回率/查全率：\n\n`实际为正例的样本中被正确预测的比例`\n$$\nrecall = \\frac{TP}{TP+FN}\n$$\n**mAP**\n\n1. 划定不同阈值计算不同的精度/召回率\n   - 计算在不同阈值的情况下，Predicision和Recall的值。\n     - 阈值0.9：无视所有小于0.9的predict，那么此时TP=1,FP=0,precision=1，所有标签数目为3，那么recall=1/3\n     - 阈值0.8：无视所有小于0.8的predict，那么此时TP=1,FP=1,precision=1/2，所有标签数目为3，那么recall=1/3\n     - 阈值0.7：无视所有小于0.7的predict，那么此时TP=2,FP=1,precision=2/3，所有标签数目为3，那么recall=2/3\n2. 根据精度/召回率，绘制RP曲线，计算**AP**值\n   - 在每个”峰值点”往左画一条直线，和上一个“峰值点”的垂直线像交，这样和坐标轴框出来的面积就是AP值。\n\n<img src=\"./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250405190809088.png\" alt=\"image-20250405190809088\" style=\"zoom:67%;\" />\n\n3. mAP：对多个类别的检测情况评估\n\n$$\nmAP=\\frac{\\sum AP}{N(classes)}\n$$\n\n#### overfeat\n\n> 时间：2013年\n>\n> 特点：采用了一种基于 **滑动窗口** 和 **全卷积神经网络（FCN）** 的方法来实现目标的分类和定位\n\n<img src=\"./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/82e1d2d67195c77b6b755473adc2d542.png\" alt=\"img\" style=\"zoom:50%;\" />\n\n##### 流程\n\n1. 通过FCN全卷积网络提取特征\n   - 首先定义若干个大小窗口（K个）\n   - K中每个窗口都要滑动图片，每个窗口都需要滑动M次\n   - 得到K x M个特征图\n2. 对每个位置的特征图进行目标分类和定位\n3. 输出每个窗口的类别得分和边框坐标\n\n#### NMS\n\n> **去除冗余的候选框**，只保留最具代表性的框，提升检测的准确性\n\n1. 标准 NMS\n   - 经典的 NMS 计算方法，直接移除冗余框\n   - 对于每个框，按得分降序排列，对所有其他框计算 IOU，并移除重叠框\n2. Soft NMS\n   - Soft NMS 不直接去除重叠的框，而是**平滑**它们的得分。具体来说，随着重叠度增加，框的得分会逐渐衰减（通过一个衰减因子）\n   - 避免移除可能是正确框的高 IOU 框，尤其是在物体边缘的框\n\n### Two-Stage\n\n#### RCNN（Region-based CNN）\n\n<img src=\"./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250405203801765.png\" alt=\"image-20250405203801765\" style=\"zoom:67%;\" />\n\n##### 流程\n\n1. 生成候选区域：基于颜色、纹理等低级特征合并超像素\n2. 统一候选区域：将每个候选区域缩放至固定尺寸\n3. 提取特征：使用预训练的CNN（如AlexNet）提特征\n4. 分类与回归：使用SVM分类/线性回归修正边界框\n\n#### SPPNet\n\n<img src=\"./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250405204318004.png\" alt=\"image-20250405204318004\" style=\"zoom:67%;\" />\n\n##### 流程\n\n1. 整图输入CNN生成特征图。\n2. 候选区域通过坐标映射到特征图对应位置。\n3. 使用SPP层（如4级金字塔：1x1、2x2、3x3、6x6）池化\n4. 分类与回归：使用SVM分类/线性回归修正边界框\n\n#### Fast RCNN\n\n<img src=\"./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250405212622317.png\" alt=\"image-20250405212622317\" style=\"zoom:67%;\" />\n\n##### 流程\n\n1. 整图输入CNN生成特征图\n2. 用**Selective Search（选择性搜索算法**）生成候选框(ROIs)\n3. RoI Pooling将不同尺寸候选框(ROIs)映射到特征图并池化为统一特征图大小\n4. 两个全连接层分别输出分类结果和边界框偏移量\n5. 多任务损失训练分类与回归网络\n\n#### ==Faster RCNN==\n\n<img src=\"./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250406141359665.png\" alt=\"image-20250406141359665\" style=\"zoom:67%;\" />\n\n<img src=\"./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250406141705239.png\" alt=\"image-20250406141705239\" style=\"zoom: 150%;\" />\n\n##### 流程\n\n1. 整图输入CNN生成特征图。\n2. **RPN**生成候选框并过滤（NMS去除低质量候选框 + 高精度低召回率 = 量少质优~300个）\n3. RoI Pooling将候选框映射到特征图并池化\n4. 两个全连接层分别输出分类结果和边界框偏移量\n5. 多任务损失训练分类与回归网络\n\n#### 对比总结\n\n<img src=\"./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250406144031898.png\" alt=\"image-20250406144031898\" style=\"zoom:67%;\" />\n\n<img src=\"./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250406144052203.png\" alt=\"image-20250406144052203\" style=\"zoom:67%;\" />\n\n<img src=\"./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250406144124733.png\" alt=\"image-20250406144124733\" style=\"zoom:67%;\" />\n\n**R-CNN网络演进：**\n\n<img src=\"./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250406144412151.png\" alt=\"image-20250406144412151\" style=\"zoom:80%;\" />\n\n#### RFCN\n\n\n\n### One-Stage\n\n#### ==SSD==\n\n<img src=\"./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250406162020930.png\" alt=\"image-20250406162020930\" style=\"zoom:67%;\" />\n\n##### 流程\n\n1. 整图输入基础网络（如VGG）生成多尺度特征图。\n2. 每个特征图位置预测K个Anchor的类别和坐标偏移。\n3. 通过NMS筛选最终检测框。\n\n#### YOLOv1\n\n`输入图片：448*448*3`\n\n![a29c47bca5ec4f359b54fc6d313879be](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/a29c47bca5ec4f359b54fc6d313879be.png)\n\n每个小框预测位置信息（`x, y, w, h, c`）+  类别概率信息`C`\n\n- x：coordinate of bbox center inside cell([0;1] wrt grid cell size)\n- y：coordinate of bbox center inside cell([0;1] wrt grid cell size)\n- w：bbox width ([0;1] wrt image)\n- h：bbox width ([0;1] wrt image)\n- c：bbox confidence ~ P(obj in bbox1)\n- C：C个不同类别概率信息\n\n#### YOLOv2\n\n\n\n#### YOLOv3\n\n\n\n#### YOLOv4\n\n\n\n#### YOLOv5\n\n![image-20250413193305078](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250413193305078.png)\n\n![image-20250413190728754](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250413190728754.png)\n\n![image-20250413190742275](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250413190742275.png)\n\n\n\n#### YOLOv6\n\n\n\n#### YOLOv7\n\n\n\n#### YOLOv8\n\n\n\n#### YOLOv12\n\n\n\n## 大模型与多模态\n\n### 概述\n\n`基本Pipeline：问题明确->数据获取->数据清理->数据探索->数据准备->训练模型->微调模型->结果应用->监控迭代`\n\n#### 整体视角\n\n- 数据决定算法的上线，模型只是去逼近这个上线\n- 算法工程师的基础能力：数据采集、评估、传输、预处理、标注、分析、挖掘、特征融合等\n\n#### LLM构建流程\n\n|          | 预训练                                             | 有监督微调                                       | 奖励建模                                 | 强化学习                             |\n| -------- | -------------------------------------------------- | ------------------------------------------------ | ---------------------------------------- | ------------------------------------ |\n| 数据集合 | 原始数据<br />[==数千亿==单词：图书、百科、网页等] | 标注用户指令<br />[==数万==用户指令和对应的答案] | 标注对比对<br />[==数万量级==标注对比对] | 用户指令<br />[==十万量级==用户指令] |\n| 算法     | 语言模型训练                                       | 语言模型训练                                     | 二分类模型                               | 强化学习方法                         |\n| 模型     | 基础模型                                           | SFT模型                                          | RM模型                                   | RL模型                               |\n| 资源需求 | 1000+GPU[月]                                       | 1-100GPU[天]                                     | 1-100GPU[天]                             | 1-100GPU[天]                         |\n\n**有监督微调：**\n\n`指令微调(Instruction Tuning)利用少量高质量数据集合，包含用户输入的提示词(Prompt)和对应的理想输出结果。用户输入包括问题、闲聊对话、任务指令等多种形式和任务`\n\n- 如何微调？利用高质量有监督数据，使用与训练阶段相同的语言模型训练算法，在基础语言模型基础上再训练，得到有监督微调模型(SFT模型)\n- 微调后的效果：具备初步指令理解能力和上下文理解能力，能够完成开放领域问题、阅读理解、翻译、生成代码等能力，也具备一定对未知任务的泛化能力\n\n**下游任务微调：**\n\n`DownstreamTaskFine-tuning`\n\n目的：在通用语义表示基础上，根据下游任务的特性进行适配\n\n注意：容易使得模型遗忘预训练阶段学习到的通用语义知识表示，损失模型的通用性和泛化能力，造成灾难性遗忘(CatastrophicForgetting)问题，因此通常采用混合预训练任务损失和下游微调损失的方法来缓解\n\n**奖励建模：**\n\n`Reward Modeling`\n\n目的：构建一个文本质量对比模型，对于同一个提示词，SFT模型给出的多个不同输出结果的质量进行排序\n\n注意：RM模型的准确率对于强化学习阶段的效果有至关重要的影响，因此需要大规模训练数据\n\n**强化学习：**\n\n`Reinforcement Learning`\n\n流程：根据数十万用户给出的提示词，利用在前一阶段训练的RM模型，给出CFT模型对用户提示词补全结果的质量评估，并与语言模型建模目标综合得到更好效果\n\n该阶段使得基础模型的熵降低，会减少模型输出的多样性\n\n1. 从数据集中sample一个prompt\n2. 语言模型(policy)生成输出\n3. 使用奖励模型(Environment)计算得分$r\\theta(x,y)$(Reward)由$r\\theta(x,y)$使用PPO-ptx算法优化语言模型\n\n<img src=\"./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250418215738655.png\" alt=\"image-20250418215738655\" style=\"zoom: 80%;\" />\n\n#### LLM参数\n\n##### 采样系数Top-k\n\n`如何预测下一个词`\n\n> 在某一解码时间步，固定选取前k个概率对应的词作为候选，并按照概率进行采样\n>\n> 采样并不代表每次都会选概率最大的，只是概率越大被选中的几率越大\n\n<img src=\"./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250419143534028.png\" alt=\"image-20250419143534028\" style=\"zoom: 67%;\" />\n\n**top-k值对解码效果影响：**\n\n- k值变大：选择范围变大，输出更加多样化但精确度也会降低\n- k值变小：输出更加确定但缺乏多样性\n\n**缺点：**\n\n- 不会更具词的概率分布动态调整k值\n\n##### 采样系数Top-p\n\n> 解决了Top-k采样中只能固定选取前k个词的问题\n>\n> 在某一解码时间步，动态选取概率之和大于p的最小集合作为候选，并按照概率进行采样\n\n<img src=\"./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250419143557550.png\" alt=\"image-20250419143557550\" style=\"zoom:67%;\" />\n\n**给定p值时，候选词列表的大小主要由概率分布决定：**\n\n- 如果模型对下一个词比较确定，则候选词列表会比较小\n- 反之，概率分布会相对均匀(对下一个词不确定)，此时候选列表会相对大一些\n\n**实际应用：**\n\n- 将`Top-k`和`Top-p`方法进行结合，先应用`Top-k`，然后应用`Top-p`\n\n##### 温度系数Temperature\n\n`控制了softmax输出分布，Temperature=1时退化为标准softmax函数`\n\n![image-20250419144841787](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250419144841787.png)\n\n**Temperature对输出结果的影响：**\n\n- 当Temperature较低时(如0.1/0.2)：模型倾向于选择概率较高的单词，生成的文本较为连贯和准确，但可能显得过于保守，缺乏创造性和多样性\n- 当Temperature较高时(如0.8/1.0)：模型倾向于选择概率较低的单词，生成的文本较为多样和创造，但可能牺牲了一定的连贯性和准确性\n\n**应用技巧：**\n\n- LLM中普遍取值一般为0.2~1.0\n- 对于多样性要求较高的任务(例如对话、文本生成)可适当提高温度系数\n\n#### 预训练模型分类\n\n##### NLU类\n\n`自然语言理解`\n\n- 以`BERT`为代表的自编码预训练模型，NLU任务：分配、抽取等\n- 如何训练？借助特定的预训练任务进行学习，如：掩码语言模型(MLM)、下一个句子预测(NSP)等\n- 双向语言模型，同时建模上文和下文信息\n- 代表模型：RoBERTa、ALBERT、ELECTRA、DeBERTa等\n- NLU任务特点：输出范围确定、评价方法相对明确\n\n##### NLG类\n\n`自然语言生成`\n\n- 以`GPT`为代表的自回归预训练模型，NLG任务：文本生成、生成式摘要、对话等\n- 如何训练？使用Causal LM训练(N-gram语言模型的自然延申)，无需设计复杂的预训练任务\n- 单向语言模型，部分模型采用双向编码器和单向编码器结构\n- 代表模型：GPT系列(Decoder-only)、T5和BART(Encoder-Decoder)等\n- NLG任务特点：输出自由度搞、评价方法较难、更具有创造性\n\n#### 模型的涌现能力\n\n`《Emergent Abilities of Large Language Models》`\n\n##### 基于模型放大\n\n- TruthfulQA：当模型放大至280B，其效果会突然高于随机20%\n- Multi-task language understanding：当训练计算量达到70B-280B后效果将远远超过随机\n- Word in Context：当PaLM被缩放至540B时，高于随机的效果出现\n\n根据文章：`Scaling Laws` for Neural Language Models\n\n<img src=\"./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250418213352077.png\" alt=\"image-20250418213352077\" style=\"zoom:67%;\" />\n\n##### 基于样例提示\n\n通过 few-shot prompting来执行任务的能力也是一种涌现现象\n\n### transformer\n\n![在这里插入图片描述](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/3319e3d6922a2e7f2499a3130d3b5925.png)\n\n- BERT（Bidirectional Encoder Representations from Transformers）：双向语言理解模型，仅使用 编码器，用于理解整个句子的上下文，适合分类、问答等理解类任务。\n- GPT（Generative Pre-trained Transformer）：自回归语言模型，仅使用 解码器，其设计目的是生成下一个词，适合用于生成式任务，如文本生成、对话等。\n\n### 多模态\n\n#### 基本结构\n\n##### vit\n\n<img src=\"./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250504143710226.png\" alt=\"image-20250504143710226\" style=\"zoom:67%;\" />\n\n##### yolos\n\n<img src=\"./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250504143729206.png\" alt=\"image-20250504143729206\" style=\"zoom: 80%;\" />\n\n#### 图文匹配\n\n##### clip\n\n<img src=\"./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250504143559792.png\" alt=\"image-20250504143559792\" style=\"zoom:67%;\" />\n\n<img src=\"./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250504150014695.png\" alt=\"image-20250504150014695\" style=\"zoom:80%;\" />\n\n![image-20250504150103155](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250504150103155.png)\n\n##### bridge tower\n\n<img src=\"./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250504160101916.png\" alt=\"image-20250504160101916\" style=\"zoom:80%;\" />\n\n<img src=\"./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250504160137868.png\" alt=\"image-20250504160137868\" style=\"zoom: 67%;\" />\n\n##### gpt4原理\n\n<img src=\"./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250504160229505.png\" alt=\"image-20250504160229505\" style=\"zoom:67%;\" />\n\n#### 文生图/图生文\n\n##### DALL · E\n\n<img src=\"./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250504154828197.png\" alt=\"image-20250504154828197\" style=\"zoom:80%;\" />\n\n<img src=\"./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250504154919514.png\" alt=\"image-20250504154919514\" style=\"zoom:80%;\" />\n\n#### 扩散模型\n\n\n\n## 模型微调\n\n`处理不当，很可能造成模型原始能力的灾难性以往、即回导致模型原始能力丢失，对于复杂模型更是如此`\n\n### 微调概念\n\n#### 全量微调\n\n- 对所有参数进行微调\n- 对算力和显存要求高\n- 效果最佳\n\n#### 局部微调\n\n> 重要调整输入输出层效果明显，而非中间层\n\n- 只调整某些**某部分参数**，例如输入层，输出层或某些特殊层\n- 对算力和显存要求一般\n- 一定是有效的\n\n#### 增量微调\n\n> 前/后增量：\n>\n> 1. 改变任务（如：2->8分类），选择后增\n> 2. 模型是一个提取特征的过程，后增效果好\n\n- 通过**新增参数**的方式进行微调，新的知识存储在新的参数中。\n- 对显存和算力要求低\n- 效果不如全量微调\n\n### 微调方式\n\n#### LoRA（Low-Rank Adaption）\n\n通过引入低秩矩阵来减少微调时的参数量。在预训练的模型中，LoRA通过添加两个小矩阵A和B来近似原始的大矩阵$\\Delta W$，从而减少需要更新的参数数量。\n$$\nW_0+\\Delta W=W_0+BA\n$$\n\n- A和B的秩远小于原始矩阵的秩，从而大大减少了需要更新的参数数量\n\n![image-20250616215925193](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B.assets/image-20250616215925193.png)\n\n- 训练时：输入分别与原始权重和两个低秩矩阵进行计算，共同的到最终结果，优化则仅优化A和B\n- 训练完成后，可以将两个低秩矩阵与原始模型中的权重进行合并，合并后的模型与原始模型无异\n\n#### QLoRA\n\n`Quantized Low-Rank Adaption，在LoRA的基础上加入量化技术，减少权重表示的位数，从而降低显存和计算需求`\n\n- 量化：将模型权重量化为低精度（如INT4），减少内存占用，并提高推理和训练速度\n\n### 微调工具\n\n#### Llama-Factory\n\n\n\n#### Xtuner\n\n\n\n### 性能评估：EvalScope\n\nhttps://evalscope.readthedocs.io/zh-cn/latest/best_practice/qwen3.html#id7\n\n## 模型部署\n\n### Ollama\n\n> - 针对个人用户\n> - 必须是gguf格式的模型【GGUF格式一般是量化后的】\n\n#### 安装\n\n**一键安装**\n\n```bash\ncurl -fsSL https://ollama.com/install.sh | sh\nsudo vim /etc/systemd/system/ollama.service\nsudo ufw allow 11434/tcp\nsudo systemctl daemon-reload & sudo systemctl restart ollama\n```\n\n**ollama.service**\n\n`使用显卡进行推理`\n\n```toml\n[Unit]\nDescription=Ollama Service\nAfter=network-online.target\n\n[Service]\nExecStart=/usr/local/bin/ollama serve\nUser=ollama\nGroup=ollama\nRestart=always\nRestartSec=3\nEnvironment=\"PATH=/mnt/big_disk_0/spa/.cargo/bin:/mnt/big_disk_0/spa/miniconda3/bin:/mnt/big_disk_0/spa/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/mnt/big_disk_0/spa/.local/bin:/mnt/big_disk_0/spa/bin\"\nEnvironment=\"OLLAMA_HOST=0.0.0.0:11434\"\nEnvironment=\"CUDA_VISIBLE_DEVICES=1,0\"\n\n[Install]\nWantedBy=default.target\n```\n\n#### 使用\n\n```bash\nollama serve\n```\n\n```bash\nollama run 模型:参数B\n```\n\n### Vllm\n\n> - 企业专用\n> - 一次只支持部署一个模型\n\n```bash\nvllm serve /mnt/big_disk/big_disk_3/spa/Code/Study/llm/XiaomiMiMo/MiMo-7B-RL \\\n\t--trust_remote_code \\\n    --tensor-parallel-size 4 \\\n    --served-model-name MiMo-7B-RL\n```\n\n### LMDeploy\n\n> - 显存优化比vllm好点\n> - 只支持列表内模型\n\n```bash\nlmdeploy serve api_server internlm/internlm2_5-7b-chat\n```","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[{"level":2,"title":"视觉模型","slug":"视觉模型","link":"#视觉模型","children":[{"level":3,"title":"分类","slug":"分类","link":"#分类","children":[]},{"level":3,"title":"参数介绍","slug":"参数介绍","link":"#参数介绍","children":[]},{"level":3,"title":"Two-Stage","slug":"two-stage","link":"#two-stage","children":[]},{"level":3,"title":"One-Stage","slug":"one-stage","link":"#one-stage","children":[]}]},{"level":2,"title":"大模型与多模态","slug":"大模型与多模态","link":"#大模型与多模态","children":[{"level":3,"title":"概述","slug":"概述","link":"#概述","children":[]},{"level":3,"title":"transformer","slug":"transformer","link":"#transformer","children":[]},{"level":3,"title":"多模态","slug":"多模态","link":"#多模态","children":[]}]},{"level":2,"title":"模型微调","slug":"模型微调","link":"#模型微调","children":[{"level":3,"title":"微调概念","slug":"微调概念","link":"#微调概念","children":[]},{"level":3,"title":"微调方式","slug":"微调方式","link":"#微调方式","children":[]},{"level":3,"title":"微调工具","slug":"微调工具","link":"#微调工具","children":[]},{"level":3,"title":"性能评估：EvalScope","slug":"性能评估-evalscope","link":"#性能评估-evalscope","children":[]}]},{"level":2,"title":"模型部署","slug":"模型部署","link":"#模型部署","children":[{"level":3,"title":"Ollama","slug":"ollama","link":"#ollama","children":[]},{"level":3,"title":"Vllm","slug":"vllm","link":"#vllm","children":[]},{"level":3,"title":"LMDeploy","slug":"lmdeploy","link":"#lmdeploy","children":[]}]}]}}
