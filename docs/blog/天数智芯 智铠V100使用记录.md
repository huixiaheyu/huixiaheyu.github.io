## 设备

- os：麒麟V10国防版
- cpu：飞腾S5000C
- gpu：智铠V100

## 配置方式

> 定制CUDA、pytorch：重新实现一遍 CUDA，伪装成 NVIDIA GPU，让 PyTorch/vLLM 以为自己在用 NVIDIA。

### 安装必要包

```bash
# centos系
yum install kernel-devel gcc make
# debian系
apt install linux-headers-$(uname -r) gcc make
```

### 下载并安装CUDA头文件

```bash
wget https://gitee.com/121786404/corex_knife/raw/master/driver/partial_install_cuda_header.tar.gz
tar xf partial_install_cuda_header.tar.gz
cd partial_install_cuda_header
bash install-cuda-header.sh 
```

### 下载并安装驱动

```bash
wget https://gitee.com/121786404/corex_knife/raw/master/driver/corex-driver-linux64-4.3.6_aarch64_10.2.run
wget https://gitee.com/121786404/corex_knife/raw/master/driver/corex.conf

/usr/local/corex/bin/corex-driver-uninstaller 卸载旧的驱动
bash corex-driver-linux64-4.3.6_aarch64_10.2.run

cp corex.conf /etc/ld.so.conf.d/
echo "export PATH=\$PATH:/usr/local/corex/bin" >> /etc/profile
rm -f /usr/local/corex
ln -s /usr/local/corex-* /usr/local/corex

ldconfig	# 
export LD_LIBRARY_PATH=/usr/local/corex/lib:/usr/local/corex/lib64:$LD_LIBRARY_PATH
export PATH=/usr/local/corex/bin/:$PATH
source /etc/profile

执行 ixsmi
可以看到GPU信息，4张BI150会显示8颗芯片
```

### 启动容器

```bash
docker run -itd \
  --name="corex_4.3.6_v102" \
  --restart=always \
  --ulimit memlock=-1:-1 \
  --privileged \
  --cap-add=ALL \
  --network=host \
  --ipc=host \
  --pid=host \
  -e LD_LIBRARY_PATH=/usr/local/corex/lib:/usr/local/corex/lib64:$LD_LIBRARY_PATH \
  -e PATH=/usr/local/corex/bin/:$PATH \
  -v /dev:/dev \
  -v /lib/modules:/lib/modules \
  -v /usr/src:/usr/src \
  -v /home/:/home \
  -v /root:/root \
  -v /mnt/sdb:/mnt/sdb \
  crpi-92uj7jb20gffz04j.cn-guangzhou.personal.cr.aliyuncs.com/iluvatar_common/vllm0.10.1-4.3.6-aarch64:v1 \
  /bin/bash
```

### 容器内执行vllm模型启动命令

> 使用python3而不是python来执行

```bash
docker exec -it corex_4.3.6_v10 bash

python3 -c "import torch;print(torch. cuda. is_available())"  # 返回true，表示pytorch可以调用到gpu了
```

## vllm

### 下载模型

```bash
# huggingface_snapshot_download.py

import argparse
import os
from huggingface_hub import snapshot_download

def download_huggingface_model(repo_id: str):
    """
    根据命令行输入的 repo_id 下载 Hugging Face 模型到指定目录，并保留 repo_id 的层级结构。
    """
    
    # 定义基础目录
    base_dir = "/mnt/sdb/modelZoo/huggingFace"
    
    # 构造目标文件夹路径，直接将 repo_id 拼接到基础目录后
    # 例如：repo_id="Qwen/Qwen3-8B" -> target_folder="/mnt/sdb/modelZoo/huggingFace/Qwen/Qwen3-8B"
    target_folder = os.path.join(base_dir, repo_id)

    # 1. 调用 snapshot_download
    print(f"开始下载 {repo_id} 到 {target_folder}...")

    try:
        os.makedirs(target_folder, exist_ok=True)
        
        download_path = snapshot_download(
            repo_id=repo_id,
            local_dir=target_folder,
            local_dir_use_symlinks=False, # 关键：确保文件直接在目标路径下
            # revision="main", # 可选：指定分支或提交版本
            # allow_patterns=["*.json", "*.model", "*.safetensors"], # 示例：只下载特定文件
            # ignore_patterns=["*.msgpack", "*.h5"], # 可选：忽略某些文件
        )
        print(f"\n✅ 下载完成！模型路径：{download_path}")
        
    except Exception as e:
        print(f"\n❌ 下载失败: {e}")
        print(f"请检查模型ID '{repo_id}' 是否正确，以及网络连接。")


if __name__ == "__main__":
    # 配置命令行参数解析器
    parser = argparse.ArgumentParser(
        description="从 Hugging Face Hub 下载模型到指定本地目录。"
    )
    
    # 添加 repo_id 参数
    parser.add_argument(
        "repo_id",
        type=str,
        help="要下载的 Hugging Face 模型ID (例如: Qwen/Qwen3-8B)"
    )

    args = parser.parse_args()
    
    # 执行下载函数
    download_huggingface_model(args.repo_id)
```

```python
# modelscope_snapshot_download.py

import argparse
import os
from modelscope import snapshot_download

def download_modelscope_model(model_id: str):
    """
    根据命令行输入的 model_id 下载 ModelScope 模型到指定目录，并保留 model_id 的层级结构。
    """
    
    # 基础目录：/mnt/sdb/modelZoo/ModelScope/
    base_dir = "/mnt/sdb/modelZoo/ModelScope"
    
    # 构造目标文件夹路径
    target_folder = os.path.join(base_dir, model_id)

    print(f"开始下载 ModelScope 模型 {model_id} 到 {target_folder}...")

    try:
        os.makedirs(target_folder, exist_ok=True)
        
        # 调用 modelscope 的 snapshot_download
        download_path = snapshot_download(
            model_id=model_id,       # ModelScope 的模型 ID
            cache_dir=target_folder, # 使用 cache_dir 来指定目标路径
            # revision="v1.0.0",     # 可选：指定版本号
            # ignore_patterns=["*.pt"], # 可选：忽略特定文件
        )
        
        # 注意：ModelScope 的 snapshot_download 返回的是缓存路径，
        # 如果您希望文件直接位于 target_folder，您可能需要在下载后进行一次移动，
        # 但通常情况下，只需将 cache_dir 视为最终的加载路径即可。
        
        print(f"\n✅ 下载完成！模型路径（实际缓存目录）：{download_path}")
        print(f"请使用此路径加载模型: {download_path}")
        
    except Exception as e:
        print(f"\n❌ 下载失败: {e}")
        print(f"请检查模型ID '{model_id}' 是否正确，以及网络连接。")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="从 ModelScope Hub 下载模型到指定本地目录。"
    )
    
    parser.add_argument(
        "model_id",
        type=str,
        help="要下载的 ModelScope 模型ID (例如: qwen/Qwen2-7B-Instruct)"
    )

    args = parser.parse_args()
    
    # 执行下载函数
    download_modelscope_model(args.model_id)
```



### 运行模型

> export HF_ENDPOINT=https://hf-mirror.com

```bash
python3 -m vllm.entrypoints.openai.api_server \
    --model /mnt/sdb/modelZoo/huggingFace/Qwen/Qwen3-8B \
    --dtype auto \
    --tensor-parallel-size 1
```

### 测试模型

```bash
# 查看支持的所有模型
curl http://localhost:8000/v1/models
# 测试接口
curl -X POST "http://10.201.8.201:8000/v1/completions" \
     -H "Content-Type: application/json" \
     -d '{
         "model": "/mnt/sdb/modelZoo/huggingFace/Qwen/Qwen3-8B",
         "prompt": "介绍一下大语言模型的工作原理。",
         "max_tokens": 100,
         "temperature": 0.2
     }'
```

```python
# main.py

from langchain_community.llms import VLLMOpenAI

llm = VLLMOpenAI(
    openai_api_key="EMPTY",                      # vLLM 服务通常不需要真实 Key，但客户端要求非空
    openai_api_base="http://10.201.8.201:8000/v1",
    model_name="/mnt/sdb/modelZoo/huggingFace/Qwen/Qwen3-8B",
    temperature=0.2,
    max_tokens=4096,
    streaming=True,
)

prompt = "介绍一下大语言模型的工作原理。"

print("=== 开始流式输出 ===")

# 使用 .stream() 方法获取生成器
try:
    for chunk in llm.stream(prompt):
        print(chunk, end="", flush=True)
except Exception as e:
    print(f"\n发生错误: {e}")

print("\n\n=== 输出结束 ===")
```

